{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d6dc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Generate a synthetic dataset\n",
    "np.random.seed(0)\n",
    "N = 100  # Number of samples\n",
    "D = 2    # Number of features\n",
    "K = 1    # Desired lower dimensionality\n",
    "\n",
    "# Generate correlated data\n",
    "mean = [0, 0]\n",
    "cov = [[1, 0.8], [0.8, 1]]\n",
    "X = np.random.multivariate_normal(mean, cov, N)\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA(n_components=K)\n",
    "Z = pca.fit_transform(X)\n",
    "\n",
    "# Plot original and projected data\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X[:, 0], X[:, 1], c='b', marker='o', label='Original Data')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Original Data')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(Z[:, 0], np.zeros(N), c='r', marker='o', label='Projected Data')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.title('Projected Data')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6392f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "num_epochs = 1000\n",
    "loss_real = -np.log(np.random.uniform(0.7, 1.0, num_epochs))\n",
    "loss_fake = -np.log(np.random.uniform(0.0, 0.3, num_epochs))\n",
    "\n",
    "# Generate x values (epochs)\n",
    "epochs = np.arange(1, num_epochs + 1)\n",
    "\n",
    "# Plot the loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, loss_real, label='Loss for Real Data')\n",
    "plt.plot(epochs, loss_fake, label='Loss for Fake/Synthesized Data')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('GAN Loss Function')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    " # Adjust layout and save the figure\n",
    "plt.tight_layout()\n",
    "\n",
    "    \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5316a6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "gan_losses = np.random.randn(1000)\n",
    "\n",
    "# Calculate the cumulative sum of the GAN losses\n",
    "cumulative_losses = np.cumsum(gan_losses)\n",
    "\n",
    "# Create the graph\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(cumulative_losses, color='b', marker='o', linestyle='-', linewidth=2, markersize=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Cumulative GAN Loss')\n",
    "plt.title('Cumulative GAN Loss Over Epochs')\n",
    "\n",
    "# Adjust layout and save the figure\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358f7c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "num_data_points = 4000\n",
    "D_psi_values = np.random.rand(num_data_points)\n",
    "D_synth_values = np.random.rand(num_data_points)\n",
    "\n",
    "# Calculate L_GAN\n",
    "L_GAN = np.sum(np.log(D_psi_values) + np.log(1 - D_synth_values))\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(1, num_data_points + 1), np.log(D_psi_values), label='log(D(|ψ_i〉))')\n",
    "plt.plot(range(1, num_data_points + 1), np.log(1 - D_synth_values), label='log(1 - D(|ψ_(synth,i)〉))')\n",
    "plt.xlabel('Data Point Index')\n",
    "plt.ylabel('Log Value')\n",
    "plt.title('L_GAN Components')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Adjust layout and save the figure\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a887d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "# Define parameters\n",
    "D = 2  # Dimensionality of the data\n",
    "num_samples = 100  # Number of data points per class\n",
    "\n",
    "# Define class means and covariance matrices\n",
    "class_means = [np.array([2, 2]), np.array([6, 6])]\n",
    "class_covariances = [np.array([[1, 0.5], [0.5, 1]]), np.array([[1, -0.5], [-0.5, 1]])]\n",
    "class_priors = [0.5, 0.5]\n",
    "\n",
    "# Generate synthetic data\n",
    "data = []\n",
    "labels = []\n",
    "for c in range(len(class_means)):\n",
    "    samples = np.random.multivariate_normal(class_means[c], class_covariances[c], num_samples)\n",
    "    data.extend(samples)\n",
    "    labels.extend([c] * num_samples)\n",
    "\n",
    "data = np.array(data)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Plot class-conditional probability densities\n",
    "x, y = np.meshgrid(np.linspace(-1, 10, 500), np.linspace(-1, 10, 500))\n",
    "pos = np.dstack((x, y))\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for c in range(len(class_means)):\n",
    "    rv = multivariate_normal(class_means[c], class_covariances[c])\n",
    "    plt.contourf(x, y, rv.pdf(pos), levels=20, alpha=0.3, cmap='coolwarm')  # Faint background\n",
    "    plt.contour(x, y, rv.pdf(pos), levels=20, alpha=0.9, colors='k')  # Brighter contours\n",
    "\n",
    "# Manually create legend entries\n",
    "handles = [plt.Line2D([0], [0], color='k', markerfacecolor='none', markersize=10, label=f'Class {c}') for c in range(len(class_means))]\n",
    "plt.legend(handles=handles)\n",
    "\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Class-Conditional Probability Densities')\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902a8456",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define the number of original features (N) and embedded dimensions (M)\n",
    "N = 5  # Replace with the actual number of original features\n",
    "M = 3  # Replace with the desired number of embedded dimensions\n",
    "\n",
    "# Generate random coefficients e_ij for mapping\n",
    "e_ij = np.random.rand(N, M)\n",
    "\n",
    "# Create a diagram to illustrate the quantum embedding\n",
    "plt.figure(figsize=(8, 6))\n",
    "for i in range(N):\n",
    "    for j in range(M):\n",
    "        plt.arrow(i, 0, 0, -1, head_width=0.1, head_length=0.1, fc='blue', ec='blue')\n",
    "        plt.arrow(i, 0, j - i, -1, head_width=0.1, head_length=0.1, fc='red', ec='red', linestyle='--')\n",
    "        plt.text(i, 0.2, f'Feature {i+1}', ha='center')\n",
    "        plt.text(j, -1.2, f'Embedded Dim {j+1}', ha='center')\n",
    "        plt.text(i + (j - i) / 2, -0.5, f'e_{i+1}{j+1}', color='red', ha='center')\n",
    "\n",
    "plt.xlim(-0.5, max(N, M) - 0.5)\n",
    "plt.ylim(-1.5, 0.5)\n",
    "plt.title('Quantum Embedding Illustration')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Embedded Dimensions')\n",
    "plt.xticks(range(max(N, M)))\n",
    "plt.yticks([])\n",
    "plt.grid(False)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1369b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define the range of values for n and d, avoiding zero and negative values\n",
    "n_values = np.arange(1, 101)  # Number of samples (n)\n",
    "d_values = np.arange(1, 101)  # Number of features (d)\n",
    "\n",
    "# Calculate time and space complexities, avoiding divide by zero\n",
    "time_complexity = np.where(d_values > 1, (n_values**3 * d_values) / np.log(d_values), 0)\n",
    "space_complexity = n_values**2 * d_values\n",
    "\n",
    "# Create a meshgrid for contour plotting\n",
    "N, D = np.meshgrid(n_values, d_values)\n",
    "Z_time = np.where(D > 1, (N**3 * D) / np.log(D), 0)\n",
    "Z_space = N**2 * D\n",
    "\n",
    "# Create subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Plot Time Complexity\n",
    "contour_time = ax1.contourf(N, D, Z_time, cmap='viridis')\n",
    "ax1.set_title('Time Complexity (O((n^3 * d) / log(d)))')\n",
    "ax1.set_xlabel('Number of Samples (n)')\n",
    "ax1.set_ylabel('Number of Features (d)')\n",
    "fig.colorbar(contour_time, ax=ax1)\n",
    "\n",
    "# Plot Space Complexity\n",
    "contour_space = ax2.contourf(N, D, Z_space, cmap='viridis')\n",
    "ax2.set_title('Space Complexity (O(n^2 * d))')\n",
    "ax2.set_xlabel('Number of Samples (n)')\n",
    "ax2.set_ylabel('Number of Features (d)')\n",
    "fig.colorbar(contour_space, ax=ax2)\n",
    "\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2d8af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define sample data for demonstration (replace with your own data)\n",
    "n = 100  # Number of data samples\n",
    "d_bar = 5  # Number of slow feature vectors\n",
    "gamma_k = 0.8  # Replace with your desired constant\n",
    "\n",
    "# Generate random magnitudes for whitened data vectors and slow feature vectors\n",
    "whitened_magnitudes = np.random.uniform(0, 1, n)\n",
    "slow_feature_magnitudes = np.random.uniform(0, 1, d_bar)\n",
    "\n",
    "# Calculate cumulative magnitudes\n",
    "cumulative_whitened = np.cumsum(whitened_magnitudes)\n",
    "cumulative_slow_feature = np.cumsum(slow_feature_magnitudes)\n",
    "\n",
    "# Create a bar chart to visualize the cumulative magnitudes\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(np.arange(n), cumulative_whitened, alpha=0.7, label='Cumulative Whitened Data')\n",
    "plt.bar(np.arange(d_bar), cumulative_slow_feature, alpha=0.7, label='Cumulative Slow Features')\n",
    "plt.axhline(gamma_k * np.sum(whitened_magnitudes), color='red', linestyle='--', label='Threshold (γ_k * ∑||z_i||)')\n",
    "plt.xlabel('Data Samples / Slow Features')\n",
    "plt.ylabel('Cumulative Magnitude')\n",
    "plt.title('Cumulative Magnitude Comparison')\n",
    "plt.legend()\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d83dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Create a meshgrid for the condition number (κ), number of samples (n), and target dimensionality (d)\n",
    "n_values = np.linspace(10, 100, 10)  # Adjust the range and number of samples as needed\n",
    "d_values = np.linspace(2, 10, 9)  # Adjust the range and number of dimensions as needed\n",
    "N, D = np.meshgrid(n_values, d_values)\n",
    "\n",
    "# Calculate the runtime of QSFA as a function of κ, n, and d (replace with your actual formula)\n",
    "runtime = N**2 * D**2 * (1 + N * D * (np.sqrt(N) + np.sqrt(D)))\n",
    "\n",
    "# Create a 3D surface plot\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "surf = ax.plot_surface(N, D, runtime, cmap='viridis')\n",
    "\n",
    "# Customize the plot\n",
    "ax.set_xlabel('Number of Samples (n)')\n",
    "ax.set_ylabel('Target Dimensionality (d)')\n",
    "ax.set_zlabel('Runtime of QSFA')\n",
    "ax.set_title('Runtime of QSFA as a Function of κ, n, and d')\n",
    "\n",
    "# Add a colorbar to indicate runtime values\n",
    "cbar = fig.colorbar(surf, ax=ax)\n",
    "cbar.set_label('Runtime')\n",
    "\n",
    "# Show the plot\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45e8a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris, load_digits, load_wine, load_breast_cancer, fetch_olivetti_faces\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load datasets\n",
    "datasets = {\n",
    "    'iris': load_iris(),\n",
    "    'digits': load_digits(),\n",
    "    'wine': load_wine(),\n",
    "    'breast_cancer': load_breast_cancer(),\n",
    "    'olivetti_faces': fetch_olivetti_faces()\n",
    "}\n",
    "\n",
    "def prepare_data(dataset):\n",
    "    X = dataset.data\n",
    "    y = dataset.target\n",
    "    return X, y\n",
    "\n",
    "def plot_dataset_distribution(X, y, ax):\n",
    "    ax.hist([X[y == label].flatten() for label in np.unique(y)], bins=30, label=np.unique(y), alpha=0.75)\n",
    "    ax.set_title('Dataset Distribution')\n",
    "    ax.legend()\n",
    "\n",
    "def plot_feature_comparison(original_features, optimized_features, ax):\n",
    "    ax.plot(original_features, label='Original Features', alpha=0.5)\n",
    "    ax.plot(optimized_features, label='Optimized Features', alpha=0.5)\n",
    "    ax.set_title('Feature Comparison')\n",
    "    ax.legend()\n",
    "\n",
    "def plot_cost_evolution(costs, ax):\n",
    "    ax.plot(costs, label='Cost Evolution')\n",
    "    ax.set_title('Cost Evolution')\n",
    "    ax.legend()\n",
    "\n",
    "def plot_variance_and_orthogonality(X, ax):\n",
    "    pca = PCA()\n",
    "    X_pca = pca.fit_transform(X)\n",
    "    variance = np.var(X_pca, axis=0)\n",
    "    ax.bar(range(len(variance)), variance)\n",
    "    ax.set_title('Variance and Orthogonality')\n",
    "\n",
    "# Setup figure and axes\n",
    "fig, axs = plt.subplots(5, 4, figsize=(20, 15))\n",
    "\n",
    "# Process each dataset\n",
    "for i, (name, dataset) in enumerate(datasets.items()):\n",
    "    X, y = prepare_data(dataset)\n",
    "    X = StandardScaler().fit_transform(X)  # Standardize the data\n",
    "    \n",
    "    # Mock optimized features and costs\n",
    "    optimized_features = np.mean(X, axis=0) + np.random.normal(size=X.shape[1])\n",
    "    costs = np.random.random(100)\n",
    "    \n",
    "    # Plot dataset distribution\n",
    "    plot_dataset_distribution(X, y, axs[i, 0])\n",
    "    \n",
    "    # Plot feature comparison\n",
    "    plot_feature_comparison(np.mean(X, axis=0), optimized_features, axs[i, 1])\n",
    "    \n",
    "    # Plot cost evolution\n",
    "    plot_cost_evolution(costs, axs[i, 2])\n",
    "    \n",
    "    # Plot variance and orthogonality\n",
    "    plot_variance_and_orthogonality(X, axs[i, 3])\n",
    "\n",
    "# Adjust layout and save figure\n",
    "plt.tight_layout()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417621d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris, load_digits, load_wine, load_breast_cancer, fetch_olivetti_faces\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load datasets\n",
    "datasets = {\n",
    "    'iris': load_iris(),\n",
    "    'digits': load_digits(),\n",
    "    'wine': load_wine(),\n",
    "    'breast_cancer': load_breast_cancer(),\n",
    "    'olivetti_faces': fetch_olivetti_faces()\n",
    "}\n",
    "\n",
    "def prepare_data(dataset):\n",
    "    X = dataset.data\n",
    "    y = dataset.target\n",
    "    return X, y\n",
    "\n",
    "def plot_dataset_distribution(X, y, ax):\n",
    "    ax.hist([X[y == label].flatten() for label in np.unique(y)], bins=30, label=np.unique(y))\n",
    "    ax.set_title('Dataset Distribution')\n",
    "    ax.set_xlabel('Value')\n",
    "    ax.set_ylabel('Frequency')\n",
    "\n",
    "\n",
    "def plot_feature_comparison(original_features, optimized_features, ax):\n",
    "    ax.plot(original_features, label='Original Features')\n",
    "    ax.plot(optimized_features, label='Optimized Features')\n",
    "    ax.set_title('Feature Comparison')\n",
    "    ax.set_xlabel('Feature Index')\n",
    "    ax.set_ylabel('Feature Value')\n",
    "\n",
    "\n",
    "def plot_cost_evolution(costs, ax):\n",
    "    ax.plot(costs, label='Cost Evolution')\n",
    "    ax.set_title('Cost Evolution')\n",
    "    ax.set_xlabel('Iteration')\n",
    "    ax.set_ylabel('Cost')\n",
    "\n",
    "\n",
    "def plot_variance_and_orthogonality(X, ax):\n",
    "    pca = PCA()\n",
    "    X_pca = pca.fit_transform(X)\n",
    "    variance = np.var(X_pca, axis=0)\n",
    "    ax.bar(range(len(variance)), variance)\n",
    "    ax.set_title('Variance and Orthogonality')\n",
    "    ax.set_xlabel('Principal Component Index')\n",
    "    ax.set_ylabel('Variance')\n",
    "\n",
    "# Setup figure and axes\n",
    "fig, axs = plt.subplots(5, 4, figsize=(20, 15))\n",
    "\n",
    "# Process each dataset\n",
    "for i, (name, dataset) in enumerate(datasets.items()):\n",
    "    X, y = prepare_data(dataset)\n",
    "    X = StandardScaler().fit_transform(X)  # Standardize the data\n",
    "    \n",
    "    # Mock optimized features and costs\n",
    "    optimized_features = np.mean(X, axis=0) + np.random.normal(size=X.shape[1])\n",
    "    costs = np.random.random(100)\n",
    "    \n",
    "    # Plot dataset distribution\n",
    "    plot_dataset_distribution(X, y, axs[i, 0])\n",
    "    \n",
    "    # Plot feature comparison\n",
    "    plot_feature_comparison(np.mean(X, axis=0), optimized_features, axs[i, 1])\n",
    "    \n",
    "    # Plot cost evolution\n",
    "    plot_cost_evolution(costs, axs[i, 2])\n",
    "    \n",
    "    # Plot variance and orthogonality\n",
    "    plot_variance_and_orthogonality(X, axs[i, 3])\n",
    "\n",
    "# Adjust layout and save figure\n",
    "plt.tight_layout()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c77d47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris, load_digits, load_wine, load_breast_cancer, fetch_olivetti_faces\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.linalg import eigh\n",
    "\n",
    "# Define qSFA functions\n",
    "def compute_covariance_matrix(data):\n",
    "    mean = np.mean(data, axis=0)\n",
    "    centered_data = data - mean\n",
    "    cov_matrix = np.cov(centered_data, rowvar=False)\n",
    "    return cov_matrix\n",
    "\n",
    "def compute_eigenvalues_eigenvectors(cov_matrix):\n",
    "    eigenvalues, eigenvectors = eigh(cov_matrix)\n",
    "    return eigenvalues, eigenvectors\n",
    "\n",
    "def project_data(data, eigenvectors, num_components):\n",
    "    return np.dot(data, eigenvectors[:, :num_components])\n",
    "\n",
    "def whiten_data(data):\n",
    "    scaler = StandardScaler()\n",
    "    whitened_data = scaler.fit_transform(data)\n",
    "    return whitened_data\n",
    "\n",
    "def slow_feature_extraction(data, num_slow_features):\n",
    "    # Compute covariance matrix\n",
    "    cov_matrix = compute_covariance_matrix(data)\n",
    "    \n",
    "    # Eigenvalue decomposition\n",
    "    eigenvalues, eigenvectors = compute_eigenvalues_eigenvectors(cov_matrix)\n",
    "    \n",
    "    # Project onto the subspace with slowest eigenvalues\n",
    "    sorted_indices = np.argsort(eigenvalues)\n",
    "    slowest_indices = sorted_indices[:num_slow_features]\n",
    "    \n",
    "    # Data projection\n",
    "    projected_data = project_data(data, eigenvectors, num_slow_features)\n",
    "    \n",
    "    return projected_data\n",
    "\n",
    "# Load datasets\n",
    "datasets = {\n",
    "    'iris': load_iris(),\n",
    "    'digits': load_digits(),\n",
    "    'wine': load_wine(),\n",
    "    'breast_cancer': load_breast_cancer(),\n",
    "    'olivetti_faces': fetch_olivetti_faces()\n",
    "}\n",
    "\n",
    "# Parameters\n",
    "num_slow_features = 2  # Number of slow features to extract\n",
    "\n",
    "# Apply qSFA to datasets\n",
    "results = {}\n",
    "for dataset_name, dataset in datasets.items():\n",
    "    data = dataset.data\n",
    "    target = dataset.target if 'target' in dataset else None\n",
    "    \n",
    "    # Whiten data\n",
    "    whitened_data = whiten_data(data)\n",
    "    \n",
    "    # Perform slow feature extraction\n",
    "    slow_features = slow_feature_extraction(whitened_data, num_slow_features)\n",
    "    \n",
    "    results[dataset_name] = {\n",
    "        'slow_features': slow_features,\n",
    "        'target': target\n",
    "    }\n",
    "\n",
    "# Visualization\n",
    "def visualize_slow_features(results, dataset_name):\n",
    "    data = results[dataset_name]['slow_features']\n",
    "    target = results[dataset_name]['target']\n",
    "    \n",
    "    if target is not None:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.scatter(data[:, 0], data[:, 1], c=target, cmap='viridis', edgecolor='k', s=50)\n",
    "        plt.colorbar(label='Target')\n",
    "        plt.title(f'Slow Features of {dataset_name} Dataset')\n",
    "        plt.xlabel('Feature 1')\n",
    "        plt.ylabel('Feature 2')\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.imshow(data, cmap='gray')\n",
    "        plt.title(f'Slow Features of {dataset_name} Dataset')\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "visualize_slow_features(results, 'iris')\n",
    "visualize_slow_features(results, 'digits')\n",
    "visualize_slow_features(results, 'wine')\n",
    "visualize_slow_features(results, 'breast_cancer')\n",
    "visualize_slow_features(results, 'olivetti_faces')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704f2bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris, load_digits, load_wine, load_breast_cancer, fetch_olivetti_faces\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.linalg import eigh\n",
    "\n",
    "# Define qSFA functions\n",
    "def compute_covariance_matrix(data):\n",
    "    mean = np.mean(data, axis=0)\n",
    "    centered_data = data - mean\n",
    "    cov_matrix = np.cov(centered_data, rowvar=False)\n",
    "    return cov_matrix\n",
    "\n",
    "def compute_eigenvalues_eigenvectors(cov_matrix):\n",
    "    eigenvalues, eigenvectors = eigh(cov_matrix)\n",
    "    return eigenvalues, eigenvectors\n",
    "\n",
    "def project_data(data, eigenvectors, num_components):\n",
    "    return np.dot(data, eigenvectors[:, :num_components])\n",
    "\n",
    "def whiten_data(data):\n",
    "    scaler = StandardScaler()\n",
    "    whitened_data = scaler.fit_transform(data)\n",
    "    return whitened_data\n",
    "\n",
    "def slow_feature_extraction(data, num_slow_features):\n",
    "    # Compute covariance matrix\n",
    "    cov_matrix = compute_covariance_matrix(data)\n",
    "    \n",
    "    # Eigenvalue decomposition\n",
    "    eigenvalues, eigenvectors = compute_eigenvalues_eigenvectors(cov_matrix)\n",
    "    \n",
    "    # Project onto the subspace with slowest eigenvalues\n",
    "    sorted_indices = np.argsort(eigenvalues)\n",
    "    slowest_indices = sorted_indices[:num_slow_features]\n",
    "    \n",
    "    # Data projection\n",
    "    projected_data = project_data(data, eigenvectors, num_slow_features)\n",
    "    \n",
    "    return projected_data\n",
    "\n",
    "# Load datasets\n",
    "datasets = {\n",
    "    'iris': load_iris(),\n",
    "    'digits': load_digits(),\n",
    "    'wine': load_wine(),\n",
    "    'breast_cancer': load_breast_cancer(),\n",
    "    'olivetti_faces': fetch_olivetti_faces()\n",
    "}\n",
    "\n",
    "# Parameters\n",
    "num_slow_features = 2  # Number of slow features to extract\n",
    "\n",
    "# Apply qSFA to datasets\n",
    "results = {}\n",
    "for dataset_name, dataset in datasets.items():\n",
    "    data = dataset.data\n",
    "    target = dataset.target if 'target' in dataset else None\n",
    "    \n",
    "    # Whiten data\n",
    "    whitened_data = whiten_data(data)\n",
    "    \n",
    "    # Perform slow feature extraction\n",
    "    slow_features = slow_feature_extraction(whitened_data, num_slow_features)\n",
    "    \n",
    "    results[dataset_name] = {\n",
    "        'slow_features': slow_features,\n",
    "        'target': target\n",
    "    }\n",
    "\n",
    "# Visualization\n",
    "fig, axs = plt.subplots(1, len(results), figsize=(20, 4))\n",
    "\n",
    "for i, (dataset_name, result) in enumerate(results.items()):\n",
    "    data = result['slow_features']\n",
    "    target = result['target']\n",
    "    \n",
    "    if target is not None:\n",
    "        scatter = axs[i].scatter(data[:, 0], data[:, 1], c=target, cmap='viridis', edgecolor='k', s=50)\n",
    "        axs[i].set_title(f'{dataset_name} Dataset')\n",
    "        axs[i].set_xlabel('Feature 1')\n",
    "        axs[i].set_ylabel('Feature 2')\n",
    "        fig.colorbar(scatter, ax=axs[i], label='Target')\n",
    "    else:\n",
    "        axs[i].imshow(data, cmap='gray')\n",
    "        axs[i].set_title(f'{dataset_name} Dataset')\n",
    "        fig.colorbar(axs[i].imshow(data, cmap='gray'), ax=axs[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3ef5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris, load_digits, load_wine, load_breast_cancer, fetch_olivetti_faces\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.linalg import svd\n",
    "\n",
    "# Load datasets\n",
    "datasets = {\n",
    "    'iris': load_iris(),\n",
    "    'digits': load_digits(),\n",
    "    'wine': load_wine(),\n",
    "    'breast_cancer': load_breast_cancer(),\n",
    "    'olivetti_faces': fetch_olivetti_faces()\n",
    "}\n",
    "\n",
    "# Normalize datasets\n",
    "def normalize_data(X):\n",
    "    scaler = StandardScaler()\n",
    "    return scaler.fit_transform(X)\n",
    "\n",
    "# Extract slow features using qSFA (classical approximation)\n",
    "def extract_slow_features(X, num_features):\n",
    "    # Normalize data\n",
    "    X = normalize_data(X)\n",
    "\n",
    "    # Compute covariance matrix\n",
    "    covariance_matrix = np.cov(X.T)\n",
    "\n",
    "    # Perform Singular Value Decomposition (SVD)\n",
    "    U, S, Vt = svd(covariance_matrix)\n",
    "    V = Vt.T\n",
    "\n",
    "    # Select the top `num_features` slowest eigenvectors\n",
    "    slow_features = V[:, :num_features]\n",
    "\n",
    "    # Project the data onto the subspace of slowest eigenvectors\n",
    "    projected_data = X @ slow_features\n",
    "\n",
    "    return projected_data\n",
    "\n",
    "# Apply qSFA to datasets\n",
    "def apply_qsfa_to_datasets(datasets, num_features=2):\n",
    "    results = {}\n",
    "    for name, data in datasets.items():\n",
    "        X = data.data\n",
    "        y = data.target\n",
    "        \n",
    "        # Extract slow features\n",
    "        projected_data = extract_slow_features(X, num_features)\n",
    "        \n",
    "        # Store results\n",
    "        results[name] = {\n",
    "            'projected_data': projected_data,\n",
    "            'target': y\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Plotting function for visualization\n",
    "def plot_qsfa_results(results):\n",
    "    fig, axes = plt.subplots(1, len(results), figsize=(20, 5))\n",
    "    for ax, (name, result) in zip(axes, results.items()):\n",
    "        projected_data = result['projected_data']\n",
    "        target = result['target']\n",
    "        \n",
    "        # Plot\n",
    "        scatter = ax.scatter(projected_data[:, 0], projected_data[:, 1], c=target, cmap='viridis', edgecolor='k')\n",
    "        ax.set_title(name)\n",
    "        ax.set_xlabel('Slow Feature 1')\n",
    "        ax.set_ylabel('Slow Feature 2')\n",
    "        \n",
    "        # Add colorbar\n",
    "        plt.colorbar(scatter, ax=ax)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Execute\n",
    "results = apply_qsfa_to_datasets(datasets)\n",
    "plot_qsfa_results(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6906ca02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import eigh\n",
    "from sklearn.datasets import load_iris, load_digits, load_wine, load_breast_cancer, fetch_olivetti_faces\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Load and preprocess datasets\n",
    "def load_and_preprocess_dataset(dataset_name, sample_size=500):\n",
    "    if dataset_name == 'iris':\n",
    "        data = load_iris()\n",
    "    elif dataset_name == 'digits':\n",
    "        data = load_digits()\n",
    "    elif dataset_name == 'wine':\n",
    "        data = load_wine()\n",
    "    elif dataset_name == 'breast_cancer':\n",
    "        data = load_breast_cancer()\n",
    "    elif dataset_name == 'olivetti_faces':\n",
    "        data = fetch_olivetti_faces()\n",
    "        data.data, data.target = data.images.reshape((data.images.shape[0], -1)), data.target\n",
    "    else:\n",
    "        raise ValueError(\"Unknown dataset name\")\n",
    "\n",
    "    X, y = data.data, data.target\n",
    "    \n",
    "    # Downsample the dataset if it's too large\n",
    "    if X.shape[0] > sample_size:\n",
    "        idx = np.random.choice(X.shape[0], sample_size, replace=False)\n",
    "        X, y = X[idx], y[idx]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Reduce dimensionality to make qPCA computationally feasible\n",
    "    pca = PCA(n_components=min(X_scaled.shape[1], 20))\n",
    "    X_reduced = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    return X_reduced, y\n",
    "\n",
    "# Quantum state tomography (qST)\n",
    "def quantum_state_tomography(psi):\n",
    "    psi = psi / np.linalg.norm(psi)  # Ensure normalization\n",
    "    rho = np.outer(psi, np.conjugate(psi))\n",
    "    return rho\n",
    "\n",
    "# Compute the covariance matrix Σ for quantum states\n",
    "def compute_covariance_matrix(quantum_states):\n",
    "    n = len(quantum_states)\n",
    "    d = quantum_states[0].shape[0]\n",
    "    \n",
    "    mean_state = np.mean(quantum_states, axis=0)\n",
    "    mean_state_flat = mean_state.flatten()\n",
    "    \n",
    "    covariance_matrix = np.zeros((d * d, d * d), dtype=np.complex128)\n",
    "    \n",
    "    for rho in quantum_states:\n",
    "        diff = rho.flatten() - mean_state_flat\n",
    "        covariance_matrix += np.outer(diff, np.conjugate(diff))\n",
    "    \n",
    "    covariance_matrix /= (n - 1)\n",
    "    return covariance_matrix\n",
    "\n",
    "# Quantum PCA (qPCA)\n",
    "def qPCA(X, n_components):\n",
    "    quantum_states = [quantum_state_tomography(x) for x in X]\n",
    "    covariance_matrix = compute_covariance_matrix(quantum_states)\n",
    "    \n",
    "    eigenvalues, eigenvectors = eigh(covariance_matrix)\n",
    "    \n",
    "    idx = np.argsort(eigenvalues)[::-1]\n",
    "    eigenvectors = eigenvectors[:, idx]\n",
    "    \n",
    "    principal_components = eigenvectors[:, :n_components]\n",
    "    \n",
    "    # Reshape X to be compatible with the principal components\n",
    "    X_reshaped = np.array([rho.flatten() for rho in quantum_states])\n",
    "    projected_data = np.dot(X_reshaped, principal_components.real)\n",
    "    \n",
    "    return projected_data, eigenvalues[idx][:n_components]\n",
    "\n",
    "# Run analysis on datasets\n",
    "def run_analysis(dataset_name, n_components=2, axs=None, row=0):\n",
    "    X, y = load_and_preprocess_dataset(dataset_name)\n",
    "    \n",
    "    # Perform qPCA on data\n",
    "    X_qPCA, qPCA_explained_variance = qPCA(X, n_components)\n",
    "    \n",
    "    # Plotting results\n",
    "    scatter_qPCA = axs[row].scatter(X_qPCA[:, 0], X_qPCA[:, 1], c=y, cmap='viridis')\n",
    "    legend1_qPCA = axs[row].legend(*scatter_qPCA.legend_elements(), title=\"Classes\")\n",
    "    axs[row].add_artist(legend1_qPCA)\n",
    "    axs[row].set_title(f'{dataset_name} - qPCA Result')\n",
    "    axs[row].set_xlabel('Principal Component 1')\n",
    "    axs[row].set_ylabel('Principal Component 2')\n",
    "    plt.colorbar(scatter_qPCA, ax=axs[row], label='Class Label')\n",
    "\n",
    "# Run the analysis for different datasets\n",
    "datasets = ['iris', 'digits', 'wine', 'breast_cancer', 'olivetti_faces']\n",
    "\n",
    "# Adjust figsize to fit all subplots in one row\n",
    "fig, axs = plt.subplots(1, len(datasets), figsize=(20, 5))\n",
    "\n",
    "for i, dataset in enumerate(datasets):\n",
    "    run_analysis(dataset, axs=axs, row=i)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ae7b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import eigh\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_iris, load_digits, load_wine, load_breast_cancer\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "\n",
    "# Quantum state tomography\n",
    "def quantum_state_tomography(psi):\n",
    "    psi = psi / np.linalg.norm(psi)\n",
    "    rho = np.outer(psi, np.conjugate(psi))\n",
    "    return rho\n",
    "\n",
    "# Compute covariance matrix\n",
    "def compute_covariance_matrix(quantum_states):\n",
    "    n = len(quantum_states)\n",
    "    d = quantum_states[0].shape[0]\n",
    "    mean_state = np.mean(quantum_states, axis=0)\n",
    "    mean_state_flat = mean_state.flatten()\n",
    "    covariance_matrix = np.zeros((d * d, d * d), dtype=np.complex128)\n",
    "    for rho in quantum_states:\n",
    "        diff = rho.flatten() - mean_state_flat\n",
    "        covariance_matrix += np.outer(diff, np.conjugate(diff))\n",
    "    covariance_matrix /= (n - 1)\n",
    "    return covariance_matrix\n",
    "\n",
    "# Quantum Linear Discriminant Analysis (qLDA)\n",
    "def qLDA(X, y, n_components, quantum_gates=None):\n",
    "    # Quantum state tomography\n",
    "    quantum_states = [quantum_state_tomography(x) for x in X]\n",
    "\n",
    "    # Compute covariance matrix\n",
    "    covariance_matrix = compute_covariance_matrix(quantum_states)\n",
    "\n",
    "    # Compute scatter matrices\n",
    "    unique_labels = np.unique(y)\n",
    "    mu = np.mean(X, axis=0)\n",
    "    S_B = np.zeros((X.shape[1], X.shape[1]), dtype=np.complex128)\n",
    "    S_W = np.zeros((X.shape[1], X.shape[1]), dtype=np.complex128)\n",
    "    \n",
    "    for label in unique_labels:\n",
    "        X_class = X[y == label]\n",
    "        mu_i = np.mean(X_class, axis=0)\n",
    "        S_B += np.outer(mu_i - mu, mu_i - mu.conj())\n",
    "        for x in X_class:\n",
    "            S_W += np.outer(x - mu_i, x - mu_i.conj())\n",
    "\n",
    "    # Regularization term\n",
    "    kappa = 1e-6\n",
    "    S_W += kappa * np.eye(S_W.shape[0])\n",
    "\n",
    "    # Solve generalized eigenvalue problem\n",
    "    eigenvalues, eigenvectors = eigh(S_B, S_W)\n",
    "    idx = np.argsort(eigenvalues)[::-1]\n",
    "    eigenvectors = eigenvectors[:, idx]\n",
    "\n",
    "    # Select top n_components\n",
    "    principal_components = eigenvectors[:, :n_components]\n",
    "\n",
    "    # Apply quantum gates if provided\n",
    "    if quantum_gates is not None:\n",
    "        principal_components = np.dot(quantum_gates, principal_components)\n",
    "\n",
    "    # Compute reduced-dimensional representation\n",
    "    Y = np.dot(X, principal_components)\n",
    "\n",
    "    return Y, eigenvalues[idx][:n_components]\n",
    "\n",
    "# Load and preprocess datasets\n",
    "def load_and_preprocess_dataset(dataset_name):\n",
    "    if dataset_name == 'iris':\n",
    "        data = load_iris()\n",
    "    elif dataset_name == 'digits':\n",
    "        data = load_digits()\n",
    "    elif dataset_name == 'wine':\n",
    "        data = load_wine()\n",
    "    elif dataset_name == 'breast_cancer':\n",
    "        data = load_breast_cancer()\n",
    "    elif dataset_name == 'olivetti_faces':\n",
    "        data = fetch_olivetti_faces()\n",
    "    else:\n",
    "        raise ValueError(\"Unknown dataset name\")\n",
    "\n",
    "    X, y = data.data, data.target\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    pca = PCA(n_components=min(X_scaled.shape[1], 20))\n",
    "    X_reduced = pca.fit_transform(X_scaled)\n",
    "    return X_reduced, y\n",
    "\n",
    "# Run analysis\n",
    "def run_analysis(dataset_name, n_components=2, axs=None, col=0):\n",
    "    X, y = load_and_preprocess_dataset(dataset_name)\n",
    "\n",
    "    # Perform qLDA on data\n",
    "    X_qLDA, qLDA_explained_variance = qLDA(X, y, n_components)\n",
    "\n",
    "    # Plotting results\n",
    "    scatter_qLDA = axs[col].scatter(X_qLDA[:, 0], X_qLDA[:, 1], c=y, cmap='viridis')\n",
    "    axs[col].set_title(f'qLDA on {dataset_name} dataset')\n",
    "    axs[col].set_xlabel('Component 1')\n",
    "    axs[col].set_ylabel('Component 2')\n",
    "    plt.colorbar(scatter_qLDA, ax=axs[col], label='Class Label')\n",
    "\n",
    "\n",
    "datasets = ['iris', 'digits', 'wine', 'breast_cancer', 'olivetti_faces']\n",
    "\n",
    "# Create subplots in one row\n",
    "fig, axs = plt.subplots(1, len(datasets), figsize=(20, 5))\n",
    "\n",
    "# Run analysis on each dataset\n",
    "for i, dataset in enumerate(datasets):\n",
    "    run_analysis(dataset, axs=axs, col=i)\n",
    "\n",
    "# Adjust layout and save plot\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269eb33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris, load_digits, load_wine, load_breast_cancer, fetch_olivetti_faces\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Load and preprocess datasets\n",
    "def load_and_preprocess_dataset(dataset_name):\n",
    "    if dataset_name == 'iris':\n",
    "        data = load_iris()\n",
    "    elif dataset_name == 'digits':\n",
    "        data = load_digits()\n",
    "    elif dataset_name == 'wine':\n",
    "        data = load_wine()\n",
    "    elif dataset_name == 'breast_cancer':\n",
    "        data = load_breast_cancer()\n",
    "    elif dataset_name == 'olivetti_faces':\n",
    "        data = fetch_olivetti_faces()\n",
    "        data.data, data.target = data.images.reshape((data.images.shape[0], -1)), data.target\n",
    "    else:\n",
    "        raise ValueError(\"Unknown dataset name\")\n",
    "\n",
    "    X, y = data.data, data.target\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    return X_scaled, y\n",
    "\n",
    "# Quantum Filter Method\n",
    "def quantum_filter(X, q_i, F, normalize=True):\n",
    "    X_filtered = np.dot(X, F)  # Ensure dimensions align\n",
    "    if normalize:\n",
    "        normalization_factor = np.sqrt(np.sum(q_i * np.dot(X.T, X)))\n",
    "        X_filtered /= normalization_factor\n",
    "    return X_filtered\n",
    "\n",
    "def quantum_filter_operator(n_features):\n",
    "    q_i = np.random.rand(n_features)\n",
    "    F = np.random.rand(n_features, n_features)  # Ensure F is (n_features, n_features)\n",
    "    return q_i, F\n",
    "\n",
    "def apply_quantum_filter(X, q_i, F):\n",
    "    X_filtered = quantum_filter(X, q_i, F)\n",
    "    return X_filtered\n",
    "\n",
    "def pca(X, n_components):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_pca = pca.fit_transform(X)\n",
    "    return X_pca, pca.explained_variance_ratio_\n",
    "\n",
    "# Implementing Quantum Filter, Wrapper, and Embedded Methods\n",
    "def quantum_filter_method(X, n_features):\n",
    "    q_i, F = quantum_filter_operator(X.shape[1])  # Use X's feature size to create F\n",
    "    X_filtered = apply_quantum_filter(X, q_i, F)\n",
    "    return X_filtered\n",
    "\n",
    "def quantum_wrapper_method(X, n_features, theta_q, U_var):\n",
    "    X_filtered = quantum_filter_method(X, n_features)\n",
    "    X_wrapped = U_var(X_filtered, theta_q)\n",
    "    return X_wrapped\n",
    "\n",
    "def quantum_embedded_method(X, n_features, lambda_e, U_emb):\n",
    "    X_filtered = quantum_filter_method(X, n_features)\n",
    "    X_embedded = U_emb(X_filtered, lambda_e)\n",
    "    return X_embedded\n",
    "\n",
    "\n",
    "def U_var(X, theta_q):\n",
    "    if len(theta_q) != X.shape[1]:\n",
    "        raise ValueError(\"Length of theta_q must match the number of features in X.\")\n",
    "    diag_matrix = np.diag(np.sin(theta_q))\n",
    "    return X @ diag_matrix\n",
    "\n",
    "def U_emb(X, lambda_e):\n",
    "    if len(lambda_e) != X.shape[1]:\n",
    "        raise ValueError(\"Length of lambda_e must match the number of features in X.\")\n",
    "    diag_matrix = np.diag(lambda_e)\n",
    "    return X @ diag_matrix\n",
    "\n",
    "def run_analysis(dataset_name, n_features, n_components=2, axs=None, row=0):\n",
    "    X, y = load_and_preprocess_dataset(dataset_name)\n",
    "    \n",
    "    # Quantum Feature Selection\n",
    "    X_filtered = quantum_filter_method(X, n_features)\n",
    "    \n",
    "    # Perform PCA on filtered data\n",
    "    X_qPCA, qPCA_explained_variance_ratio = pca(X_filtered, n_components)\n",
    "    \n",
    "    # Additional analysis (e.g., using Wrapper and Embedded methods)\n",
    "    X_qWrapper = quantum_wrapper_method(X, X.shape[1], theta_q=np.linspace(0, np.pi, X.shape[1]), U_var=U_var)\n",
    "    X_qEmbedded = quantum_embedded_method(X, X.shape[1], lambda_e=np.ones(X.shape[1]), U_emb=U_emb)\n",
    "\n",
    "    # Plotting results\n",
    "    axs[row, 0].bar(range(n_features), np.random.rand(n_features))\n",
    "    axs[row, 0].set_title(f'{dataset_name} - Feature Importance (Quantum Scoring)')\n",
    "    axs[row, 0].set_xlabel('Feature Index')\n",
    "    axs[row, 0].set_ylabel('Score')\n",
    "\n",
    "    scatter_qPCA = axs[row, 1].scatter(X_qPCA[:, 0], X_qPCA[:, 1], c=y, cmap='viridis')\n",
    "    legend1_qPCA = axs[row, 1].legend(*scatter_qPCA.legend_elements(), title=\"Classes\")\n",
    "    axs[row, 1].add_artist(legend1_qPCA)\n",
    "    axs[row, 1].set_title(f'{dataset_name} - qPCA Result')\n",
    "    axs[row, 1].set_xlabel('Principal Component 1')\n",
    "    axs[row, 1].set_ylabel('Principal Component 2')\n",
    "    #plt.colorbar(scatter_qPCA, ax=axs[row, 1], label='Class Label')\n",
    "\n",
    "    scatter_qWrapper = axs[row, 2].scatter(X_qWrapper[:, 0], X_qWrapper[:, 1], c=y, cmap='viridis')\n",
    "    legend1_qWrapper = axs[row, 2].legend(*scatter_qWrapper.legend_elements(), title=\"Classes\")\n",
    "    axs[row, 2].add_artist(legend1_qWrapper)\n",
    "    axs[row, 2].set_title(f'{dataset_name} - qWrapper Result')\n",
    "    axs[row, 2].set_xlabel('Feature 1')\n",
    "    axs[row, 2].set_ylabel('Feature 2')\n",
    "    #plt.colorbar(scatter_qWrapper, ax=axs[row, 2], label='Class Label')\n",
    "\n",
    "    scatter_qEmbedded = axs[row, 3].scatter(X_qEmbedded[:, 0], X_qEmbedded[:, 1], c=y, cmap='viridis')\n",
    "    legend1_qEmbedded = axs[row, 3].legend(*scatter_qEmbedded.legend_elements(), title=\"Classes\")\n",
    "    axs[row, 3].add_artist(legend1_qEmbedded)\n",
    "    axs[row, 3].set_title(f'{dataset_name} - qEmbedded Result')\n",
    "    axs[row, 3].set_xlabel('Feature 1')\n",
    "    axs[row, 3].set_ylabel('Feature 2')\n",
    "    plt.colorbar(scatter_qEmbedded, ax=axs[row, 3], label='Class Label')\n",
    "\n",
    "# Run the analysis for different datasets\n",
    "datasets = ['iris', 'digits', 'wine', 'breast_cancer', 'olivetti_faces']\n",
    "n_features = 10\n",
    "fig, axs = plt.subplots(len(datasets), 4, figsize=(24, 5 * len(datasets)))\n",
    "\n",
    "for i, dataset in enumerate(datasets):\n",
    "    run_analysis(dataset, n_features, axs=axs, row=i)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785e114f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import eigh\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_iris, load_digits, load_wine, load_breast_cancer\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "\n",
    "# Quantum state tomography\n",
    "def quantum_state_tomography(psi):\n",
    "    psi = psi / np.linalg.norm(psi)\n",
    "    rho = np.outer(psi, np.conjugate(psi))\n",
    "    return rho\n",
    "\n",
    "# Compute covariance matrix\n",
    "def compute_covariance_matrix(quantum_states):\n",
    "    n = len(quantum_states)\n",
    "    d = quantum_states[0].shape[0]\n",
    "    mean_state = np.mean(quantum_states, axis=0)\n",
    "    mean_state_flat = mean_state.flatten()\n",
    "    covariance_matrix = np.zeros((d * d, d * d), dtype=np.complex128)\n",
    "    for rho in quantum_states:\n",
    "        diff = rho.flatten() - mean_state_flat\n",
    "        covariance_matrix += np.outer(diff, np.conjugate(diff))\n",
    "    covariance_matrix /= (n - 1)\n",
    "    return covariance_matrix\n",
    "\n",
    "# Quantum Gaussian Distribution Adaptation (qGDA)\n",
    "def qGDA(X, y, n_components, learning_rate=0.01, n_iterations=100):\n",
    "    n, d = X.shape\n",
    "\n",
    "    # Initialize quantum states\n",
    "    quantum_states = [quantum_state_tomography(x) for x in X]\n",
    "\n",
    "    # Compute covariance matrix\n",
    "    covariance_matrix = compute_covariance_matrix(quantum_states)\n",
    "\n",
    "    # Initialize parameters (e.g., random unitary matrix)\n",
    "    theta = np.random.rand(d, d)\n",
    "    \n",
    "    for _ in range(n_iterations):\n",
    "        # Compute objective function and its gradient\n",
    "        objective_function, gradient = compute_gradient(X, y, theta, covariance_matrix)\n",
    "        \n",
    "        # Update parameters\n",
    "        theta -= learning_rate * gradient\n",
    "\n",
    "    # Perform dimensionality reduction\n",
    "    principal_components = np.linalg.svd(theta, full_matrices=False)[0][:, :n_components]\n",
    "    Y = np.dot(X, principal_components)\n",
    "\n",
    "    return Y\n",
    "\n",
    "# Compute gradient of the objective function\n",
    "def compute_gradient(X, y, theta, covariance_matrix):\n",
    "\n",
    "    gradient = np.random.rand(*theta.shape)  # Replace with actual gradient calculation\n",
    "    objective_function = 0  # Replace with actual objective function value\n",
    "    return objective_function, gradient\n",
    "\n",
    "# Load and preprocess datasets\n",
    "def load_and_preprocess_dataset(dataset_name):\n",
    "    if dataset_name == 'iris':\n",
    "        data = load_iris()\n",
    "    elif dataset_name == 'digits':\n",
    "        data = load_digits()\n",
    "    elif dataset_name == 'wine':\n",
    "        data = load_wine()\n",
    "    elif dataset_name == 'breast_cancer':\n",
    "        data = load_breast_cancer()\n",
    "    elif dataset_name == 'olivetti_faces':\n",
    "        data = fetch_olivetti_faces()\n",
    "    else:\n",
    "        raise ValueError(\"Unknown dataset name\")\n",
    "\n",
    "    X, y = data.data, data.target\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    pca = PCA(n_components=min(X_scaled.shape[1], 20))\n",
    "    X_reduced = pca.fit_transform(X_scaled)\n",
    "    return X_reduced, y\n",
    "\n",
    "# Run analysis\n",
    "def run_analysis(dataset_name, n_components=2, axs=None, col=0):\n",
    "    X, y = load_and_preprocess_dataset(dataset_name)\n",
    "\n",
    "    # Perform qGDA on data\n",
    "    X_qGDA = qGDA(X, y, n_components)\n",
    "\n",
    "    # Plotting results\n",
    "    scatter_qGDA = axs[col].scatter(X_qGDA[:, 0], X_qGDA[:, 1], c=y, cmap='viridis')\n",
    "    axs[col].set_title(f'qGDA on {dataset_name} dataset')\n",
    "    axs[col].set_xlabel('Component 1')\n",
    "    axs[col].set_ylabel('Component 2')\n",
    "    plt.colorbar(scatter_qGDA, ax=axs[col], label='Class Label')\n",
    "\n",
    "\n",
    "datasets = ['iris', 'digits', 'wine', 'breast_cancer', 'olivetti_faces']\n",
    "\n",
    "# Create subplots in one row\n",
    "fig, axs = plt.subplots(1, len(datasets), figsize=(20, 5))\n",
    "\n",
    "# Run analysis on each dataset\n",
    "for i, dataset in enumerate(datasets):\n",
    "    run_analysis(dataset, axs=axs, col=i)\n",
    "\n",
    "# Adjust layout and save plot\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e745b72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_iris, load_digits, load_wine, load_breast_cancer, fetch_olivetti_faces\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Quantum state preparation\n",
    "def prepare_quantum_state(X):\n",
    "    return X / np.linalg.norm(X, axis=1, keepdims=True)\n",
    "\n",
    "# Encoding quantum state |ϕ_i ⟩ based on the provided equation\n",
    "def encode_quantum_state(X, U, alpha, beta, H):\n",
    "    UX_T = U @ X.T\n",
    "    encoded = UX_T.copy()\n",
    "    encoded += np.outer(alpha, np.ones(UX_T.shape[1])) * UX_T\n",
    "    encoded += beta * (U @ H @ X.T)\n",
    "    return encoded.T\n",
    "\n",
    "# Decoding quantum state |ψ_(rec,i) ⟩ based on the provided equation\n",
    "def decode_quantum_state(ϕ, V, gamma, delta, H):\n",
    "    Vϕ_T = V @ ϕ.T\n",
    "    decoded = Vϕ_T.copy()\n",
    "    decoded += np.outer(gamma, np.ones(Vϕ_T.shape[1])) * Vϕ_T\n",
    "    decoded += delta * (V @ H @ ϕ.T)\n",
    "    return decoded.T\n",
    "\n",
    "# Optimization of qAE to minimize reconstruction error\n",
    "def optimize_qAE(X, U, V, alpha, beta, gamma, delta, H, n_iterations=50, learning_rate=0.001):\n",
    "    for _ in range(n_iterations):\n",
    "        ϕ = encode_quantum_state(X, U, alpha, beta, H)\n",
    "        X_rec = decode_quantum_state(ϕ, V, gamma, delta, H)\n",
    "        \n",
    "        # Simplified gradient update (replace with actual gradient calculation)\n",
    "        grad_U = np.random.randn(*U.shape) * 0.01\n",
    "        grad_V = np.random.randn(*V.shape) * 0.01\n",
    "        \n",
    "        U -= learning_rate * grad_U\n",
    "        V -= learning_rate * grad_V\n",
    "\n",
    "    return U, V\n",
    "\n",
    "# Fidelity calculation for preservation\n",
    "# Fidelity calculation for preservation\n",
    "def fidelity_preservation(X, ϕ, U, H, alpha, beta):\n",
    "    X = X.reshape(X.shape[0], -1)\n",
    "    ϕ = ϕ.reshape(ϕ.shape[0], -1)\n",
    "\n",
    "    F1 = np.einsum('ij,ij->i', X, ϕ)\n",
    "    F2 = np.einsum('ij,jk->i', X, U @ X.T)\n",
    "    F3 = beta * np.einsum('ij,jk->i', X, H @ ϕ.T)\n",
    "    \n",
    "    # Ensure alpha and F2 dimensions align by broadcasting alpha\n",
    "    alpha_resized = np.resize(alpha, F2.shape)  # Resize alpha to match F2\n",
    "    F2_alpha = alpha_resized * F2\n",
    "    \n",
    "    return np.abs(F1 + F2_alpha + F3) ** 2\n",
    "\n",
    "# Fidelity calculation for reconstruction\n",
    "# Fidelity calculation for reconstruction\n",
    "def fidelity_reconstruction(X, X_rec, V, H, gamma, delta):\n",
    "    X = X.reshape(X.shape[0], -1)\n",
    "    X_rec = X_rec.reshape(X_rec.shape[0], -1)\n",
    "\n",
    "    F1 = np.einsum('ij,ij->i', X_rec, X)\n",
    "    F2 = np.einsum('ij,jk->i', X_rec, V @ X.T)\n",
    "    F3 = delta * np.einsum('ij,jk->i', X_rec, H @ V @ X.T)\n",
    "    \n",
    "    # Ensure gamma and F2 dimensions align by broadcasting gamma\n",
    "    gamma_resized = np.resize(gamma, F2.shape)  # Resize gamma to match F2\n",
    "    F2_gamma = gamma_resized * F2\n",
    "    \n",
    "    return np.abs(F1 + F2_gamma + F3) ** 2\n",
    "\n",
    "\n",
    "# Cost function based on fidelity\n",
    "def cost_function(F):\n",
    "    return np.sum(1 - F)\n",
    "\n",
    "# Load and preprocess dataset\n",
    "def load_and_preprocess_data(dataset_name):\n",
    "    if dataset_name == 'iris':\n",
    "        data = load_iris()\n",
    "    elif dataset_name == 'digits':\n",
    "        data = load_digits()\n",
    "    elif dataset_name == 'wine':\n",
    "        data = load_wine()\n",
    "    elif dataset_name == 'breast_cancer':\n",
    "        data = load_breast_cancer()\n",
    "    elif dataset_name == 'olivetti_faces':\n",
    "        data = fetch_olivetti_faces()\n",
    "    else:\n",
    "        raise ValueError(\"Unknown dataset name\")\n",
    "\n",
    "    X, y = data.data, data.target\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    return X, y\n",
    "\n",
    "# Run qAE on multiple datasets and plot fidelity\n",
    "def run_qAE_on_datasets(datasets):\n",
    "    fig, axs = plt.subplots(1, len(datasets), figsize=(20, 5))\n",
    "\n",
    "    for i, dataset in enumerate(datasets):\n",
    "        X, y = load_and_preprocess_data(dataset)\n",
    "        X = prepare_quantum_state(X)\n",
    "\n",
    "        n, d = X.shape\n",
    "        U = np.random.randn(d, d)  # Encoding channel U\n",
    "        V = np.random.randn(d, d)  # Decoding channel V\n",
    "        alpha = np.random.randn(d)\n",
    "        beta = np.random.rand()\n",
    "        gamma = np.random.randn(d)\n",
    "        delta = np.random.rand()\n",
    "        H = np.random.randn(d, d)  # Hermitian operator H\n",
    "\n",
    "        U, V = optimize_qAE(X, U, V, alpha, beta, gamma, delta, H)\n",
    "\n",
    "        ϕ = encode_quantum_state(X, U, alpha, beta, H)\n",
    "        X_rec = decode_quantum_state(ϕ, V, gamma, delta, H)\n",
    "\n",
    "        F_preservation = fidelity_preservation(X, ϕ, U, H, alpha, beta)\n",
    "        F_reconstruction = fidelity_reconstruction(X, X_rec, V, H, gamma, delta)\n",
    "        cost = cost_function(F_reconstruction)\n",
    "\n",
    "        scatter = axs[i].scatter(X_rec[:, 0], X_rec[:, 1], c=y, cmap='viridis')\n",
    "        axs[i].set_title(f'{dataset}: Fidelity\\n {np.mean(F_reconstruction):.4f}, \\nCost {cost:.4f}')\n",
    "        axs[i].set_xlabel('Component 1')\n",
    "        axs[i].set_ylabel('Component 2')\n",
    "\n",
    "        plt.colorbar(scatter, ax=axs[i], orientation='vertical')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Datasets to analyze\n",
    "datasets = ['iris', 'digits', 'wine', 'breast_cancer', 'olivetti_faces']\n",
    "\n",
    "# Run the qAE analysis\n",
    "run_qAE_on_datasets(datasets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2395b6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_iris, load_digits, load_wine, load_breast_cancer, fetch_olivetti_faces\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Quantum state preparation\n",
    "def prepare_quantum_state(X):\n",
    "    return X / np.linalg.norm(X, axis=1, keepdims=True)\n",
    "\n",
    "# Encoding quantum state |ϕ_i ⟩ based on the provided equation\n",
    "def encode_quantum_state(X, U, alpha, beta, H):\n",
    "    UX_T = U @ X.T\n",
    "    encoded = UX_T.copy()\n",
    "    encoded += np.outer(alpha, np.ones(UX_T.shape[1])) * UX_T\n",
    "    encoded += beta * (U @ H @ X.T)\n",
    "    return encoded.T\n",
    "\n",
    "# Decoding quantum state |ψ_(rec,i) ⟩ based on the provided equation\n",
    "def decode_quantum_state(ϕ, V, gamma, delta, H):\n",
    "    Vϕ_T = V @ ϕ.T\n",
    "    decoded = Vϕ_T.copy()\n",
    "    decoded += np.outer(gamma, np.ones(Vϕ_T.shape[1])) * Vϕ_T\n",
    "    decoded += delta * (V @ H @ ϕ.T)\n",
    "    return decoded.T\n",
    "\n",
    "# Optimization of qAE to minimize reconstruction error\n",
    "def optimize_qAE(X, U, V, alpha, beta, gamma, delta, H, n_iterations=50, learning_rate=0.001):\n",
    "    for _ in range(n_iterations):\n",
    "        ϕ = encode_quantum_state(X, U, alpha, beta, H)\n",
    "        X_rec = decode_quantum_state(ϕ, V, gamma, delta, H)\n",
    "        \n",
    "        # Simplified gradient update (replace with actual gradient calculation)\n",
    "        grad_U = np.random.randn(*U.shape) * 0.01\n",
    "        grad_V = np.random.randn(*V.shape) * 0.01\n",
    "        \n",
    "        U -= learning_rate * grad_U\n",
    "        V -= learning_rate * grad_V\n",
    "\n",
    "    return U, V\n",
    "\n",
    "# Fidelity calculation for preservation\n",
    "# Fidelity calculation for preservation\n",
    "def fidelity_preservation(X, ϕ, U, H, alpha, beta):\n",
    "    X = X.reshape(X.shape[0], -1)\n",
    "    ϕ = ϕ.reshape(ϕ.shape[0], -1)\n",
    "\n",
    "    F1 = np.einsum('ij,ij->i', X, ϕ)\n",
    "    F2 = np.einsum('ij,jk->i', X, U @ X.T)\n",
    "    F3 = beta * np.einsum('ij,jk->i', X, H @ ϕ.T)\n",
    "    \n",
    "    # Ensure alpha and F2 dimensions align by broadcasting alpha\n",
    "    alpha_resized = np.resize(alpha, F2.shape)  # Resize alpha to match F2\n",
    "    F2_alpha = alpha_resized * F2\n",
    "    \n",
    "    return np.abs(F1 + F2_alpha + F3) ** 2\n",
    "\n",
    "# Fidelity calculation for reconstruction\n",
    "# Fidelity calculation for reconstruction\n",
    "def fidelity_reconstruction(X, X_rec, V, H, gamma, delta):\n",
    "    X = X.reshape(X.shape[0], -1)\n",
    "    X_rec = X_rec.reshape(X_rec.shape[0], -1)\n",
    "\n",
    "    F1 = np.einsum('ij,ij->i', X_rec, X)\n",
    "    F2 = np.einsum('ij,jk->i', X_rec, V @ X.T)\n",
    "    F3 = delta * np.einsum('ij,jk->i', X_rec, H @ V @ X.T)\n",
    "    \n",
    "    # Ensure gamma and F2 dimensions align by broadcasting gamma\n",
    "    gamma_resized = np.resize(gamma, F2.shape)  # Resize gamma to match F2\n",
    "    F2_gamma = gamma_resized * F2\n",
    "    \n",
    "    return np.abs(F1 + F2_gamma + F3) ** 2\n",
    "\n",
    "\n",
    "# Cost function based on fidelity\n",
    "def cost_function(F):\n",
    "    return np.sum(1 - F)\n",
    "\n",
    "# Load and preprocess dataset\n",
    "def load_and_preprocess_data(dataset_name):\n",
    "    if dataset_name == 'iris':\n",
    "        data = load_iris()\n",
    "    elif dataset_name == 'digits':\n",
    "        data = load_digits()\n",
    "    elif dataset_name == 'wine':\n",
    "        data = load_wine()\n",
    "    elif dataset_name == 'breast_cancer':\n",
    "        data = load_breast_cancer()\n",
    "    elif dataset_name == 'olivetti_faces':\n",
    "        data = fetch_olivetti_faces()\n",
    "    else:\n",
    "        raise ValueError(\"Unknown dataset name\")\n",
    "\n",
    "    X, y = data.data, data.target\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    return X, y\n",
    "\n",
    "# Run qAE on multiple datasets and plot fidelity\n",
    "def run_qAE_on_datasets(datasets):\n",
    "    fig, axs = plt.subplots(1, len(datasets), figsize=(20, 5))\n",
    "\n",
    "    for i, dataset in enumerate(datasets):\n",
    "        X, y = load_and_preprocess_data(dataset)\n",
    "        X = prepare_quantum_state(X)\n",
    "\n",
    "        n, d = X.shape\n",
    "        U = np.random.randn(d, d)  # Encoding channel U\n",
    "        V = np.random.randn(d, d)  # Decoding channel V\n",
    "        alpha = np.random.randn(d)\n",
    "        beta = np.random.rand()\n",
    "        gamma = np.random.randn(d)\n",
    "        delta = np.random.rand()\n",
    "        H = np.random.randn(d, d)  # Hermitian operator H\n",
    "\n",
    "        U, V = optimize_qAE(X, U, V, alpha, beta, gamma, delta, H)\n",
    "\n",
    "        ϕ = encode_quantum_state(X, U, alpha, beta, H)\n",
    "        X_rec = decode_quantum_state(ϕ, V, gamma, delta, H)\n",
    "\n",
    "        F_preservation = fidelity_preservation(X, ϕ, U, H, alpha, beta)\n",
    "        F_reconstruction = fidelity_reconstruction(X, X_rec, V, H, gamma, delta)\n",
    "        cost = cost_function(F_reconstruction)\n",
    "\n",
    "        scatter = axs[i].scatter(X_rec[:, 0], X_rec[:, 1], c=y, cmap='viridis')\n",
    "        axs[i].set_title(f'{dataset}: Fidelity\\n {np.mean(F_reconstruction):.4f}, \\nCost {cost:.4f}')\n",
    "        axs[i].set_xlabel('Component 1')\n",
    "        axs[i].set_ylabel('Component 2')\n",
    "\n",
    "        plt.colorbar(scatter, ax=axs[i], orientation='vertical')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Datasets to analyze\n",
    "datasets = ['iris', 'digits', 'wine', 'breast_cancer', 'olivetti_faces']\n",
    "\n",
    "# Run the qAE analysis\n",
    "run_qAE_on_datasets(datasets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dde403a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_iris, load_digits, load_wine, load_breast_cancer, fetch_olivetti_faces\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Quantum state preparation\n",
    "def prepare_quantum_state(X):\n",
    "    return X / np.linalg.norm(X, axis=1, keepdims=True)\n",
    "\n",
    "# Encoding quantum state |ϕ_i ⟩ based on the provided equation\n",
    "def encode_quantum_state(X, U, alpha, beta, H):\n",
    "    UX_T = U @ X.T\n",
    "    encoded = UX_T.copy()\n",
    "    encoded += np.outer(alpha, np.ones(UX_T.shape[1])) * UX_T\n",
    "    encoded += beta * (U @ H @ X.T)\n",
    "    return encoded.T\n",
    "\n",
    "# Decoding quantum state |ψ_(rec,i) ⟩ based on the provided equation\n",
    "def decode_quantum_state(ϕ, V, gamma, delta, H):\n",
    "    Vϕ_T = V @ ϕ.T\n",
    "    decoded = Vϕ_T.copy()\n",
    "    decoded += np.outer(gamma, np.ones(Vϕ_T.shape[1])) * Vϕ_T\n",
    "    decoded += delta * (V @ H @ ϕ.T)\n",
    "    return decoded.T\n",
    "\n",
    "# Optimization of qAE to minimize reconstruction error\n",
    "def optimize_qAE(X, U, V, alpha, beta, gamma, delta, H, n_iterations=50, learning_rate=0.001):\n",
    "    for _ in range(n_iterations):\n",
    "        ϕ = encode_quantum_state(X, U, alpha, beta, H)\n",
    "        X_rec = decode_quantum_state(ϕ, V, gamma, delta, H)\n",
    "        \n",
    "        # Simplified gradient update (replace with actual gradient calculation)\n",
    "        grad_U = np.random.randn(*U.shape) * 0.01\n",
    "        grad_V = np.random.randn(*V.shape) * 0.01\n",
    "        \n",
    "        U -= learning_rate * grad_U\n",
    "        V -= learning_rate * grad_V\n",
    "\n",
    "    return U, V\n",
    "\n",
    "# Fidelity calculation for preservation\n",
    "# Fidelity calculation for preservation\n",
    "def fidelity_preservation(X, ϕ, U, H, alpha, beta):\n",
    "    X = X.reshape(X.shape[0], -1)\n",
    "    ϕ = ϕ.reshape(ϕ.shape[0], -1)\n",
    "\n",
    "    F1 = np.einsum('ij,ij->i', X, ϕ)\n",
    "    F2 = np.einsum('ij,jk->i', X, U @ X.T)\n",
    "    F3 = beta * np.einsum('ij,jk->i', X, H @ ϕ.T)\n",
    "    \n",
    "    # Ensure alpha and F2 dimensions align by broadcasting alpha\n",
    "    alpha_resized = np.resize(alpha, F2.shape)  # Resize alpha to match F2\n",
    "    F2_alpha = alpha_resized * F2\n",
    "    \n",
    "    return np.abs(F1 + F2_alpha + F3) ** 2\n",
    "\n",
    "# Fidelity calculation for reconstruction\n",
    "# Fidelity calculation for reconstruction\n",
    "def fidelity_reconstruction(X, X_rec, V, H, gamma, delta):\n",
    "    X = X.reshape(X.shape[0], -1)\n",
    "    X_rec = X_rec.reshape(X_rec.shape[0], -1)\n",
    "\n",
    "    F1 = np.einsum('ij,ij->i', X_rec, X)\n",
    "    F2 = np.einsum('ij,jk->i', X_rec, V @ X.T)\n",
    "    F3 = delta * np.einsum('ij,jk->i', X_rec, H @ V @ X.T)\n",
    "    \n",
    "    # Ensure gamma and F2 dimensions align by broadcasting gamma\n",
    "    gamma_resized = np.resize(gamma, F2.shape)  # Resize gamma to match F2\n",
    "    F2_gamma = gamma_resized * F2\n",
    "    \n",
    "    return np.abs(F1 + F2_gamma + F3) ** 2\n",
    "\n",
    "\n",
    "# Cost function based on fidelity\n",
    "def cost_function(F):\n",
    "    return np.sum(1 - F)\n",
    "\n",
    "# Load and preprocess dataset\n",
    "def load_and_preprocess_data(dataset_name):\n",
    "    if dataset_name == 'iris':\n",
    "        data = load_iris()\n",
    "    elif dataset_name == 'digits':\n",
    "        data = load_digits()\n",
    "    elif dataset_name == 'wine':\n",
    "        data = load_wine()\n",
    "    elif dataset_name == 'breast_cancer':\n",
    "        data = load_breast_cancer()\n",
    "    elif dataset_name == 'olivetti_faces':\n",
    "        data = fetch_olivetti_faces()\n",
    "    else:\n",
    "        raise ValueError(\"Unknown dataset name\")\n",
    "\n",
    "    X, y = data.data, data.target\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    return X, y\n",
    "\n",
    "# Run qAE on multiple datasets and plot fidelity\n",
    "def run_qAE_on_datasets(datasets):\n",
    "    fig, axs = plt.subplots(1, len(datasets), figsize=(20, 5))\n",
    "\n",
    "    for i, dataset in enumerate(datasets):\n",
    "        X, y = load_and_preprocess_data(dataset)\n",
    "        X = prepare_quantum_state(X)\n",
    "\n",
    "        n, d = X.shape\n",
    "        U = np.random.randn(d, d)  # Encoding channel U\n",
    "        V = np.random.randn(d, d)  # Decoding channel V\n",
    "        alpha = np.random.randn(d)\n",
    "        beta = np.random.rand()\n",
    "        gamma = np.random.randn(d)\n",
    "        delta = np.random.rand()\n",
    "        H = np.random.randn(d, d)  # Hermitian operator H\n",
    "\n",
    "        U, V = optimize_qAE(X, U, V, alpha, beta, gamma, delta, H)\n",
    "\n",
    "        ϕ = encode_quantum_state(X, U, alpha, beta, H)\n",
    "        X_rec = decode_quantum_state(ϕ, V, gamma, delta, H)\n",
    "\n",
    "        F_preservation = fidelity_preservation(X, ϕ, U, H, alpha, beta)\n",
    "        F_reconstruction = fidelity_reconstruction(X, X_rec, V, H, gamma, delta)\n",
    "        cost = cost_function(F_reconstruction)\n",
    "\n",
    "        scatter = axs[i].scatter(X_rec[:, 0], X_rec[:, 1], c=y, cmap='viridis')\n",
    "        axs[i].set_title(f'{dataset}: Fidelity\\n {np.mean(F_reconstruction):.4f}, \\nCost {cost:.4f}')\n",
    "        axs[i].set_xlabel('Component 1')\n",
    "        axs[i].set_ylabel('Component 2')\n",
    "\n",
    "        plt.colorbar(scatter, ax=axs[i], orientation='vertical')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Datasets to analyze\n",
    "datasets = ['iris', 'digits', 'wine', 'breast_cancer', 'olivetti_faces']\n",
    "\n",
    "# Run the qAE analysis\n",
    "run_qAE_on_datasets(datasets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cc0d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_iris, load_digits, load_wine, load_breast_cancer, fetch_olivetti_faces\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Function to encode classical data into quantum states\n",
    "def encode_classical_to_quantum(X):\n",
    "    return X / np.linalg.norm(X, axis=1, keepdims=True)\n",
    "\n",
    "# Quantum generative model representation with parameterized unitary transformations\n",
    "def qGM(U, V, psi, alpha, beta):\n",
    "    term1 = np.sum([alpha[j] * (U[j] @ psi @ U[j].conj().T) for j in range(len(alpha))], axis=0)\n",
    "    term2 = np.sum([beta[k] * (V[k] @ psi @ V[k].conj().T) for k in range(len(beta))], axis=0)\n",
    "    return term1 + term2\n",
    "\n",
    "# Fidelity measurement for feature preservation\n",
    "def fidelity_measurement(psi_x, G_theta_psi_x):\n",
    "    return np.abs(np.trace(psi_x @ G_theta_psi_x.conj().T))**2\n",
    "\n",
    "# Loss function for optimization of qGM\n",
    "def loss_function(F_preservation, F_reconstruction, gamma, delta):\n",
    "    return gamma * (1 - F_preservation) + delta * (1 - F_reconstruction)\n",
    "\n",
    "# Load and preprocess dataset\n",
    "def load_and_preprocess_data(dataset_name, n_components=None):\n",
    "    if dataset_name == 'iris':\n",
    "        data = load_iris()\n",
    "    elif dataset_name == 'digits':\n",
    "        data = load_digits()\n",
    "    elif dataset_name == 'wine':\n",
    "        data = load_wine()\n",
    "    elif dataset_name == 'breast_cancer':\n",
    "        data = load_breast_cancer()\n",
    "    elif dataset_name == 'olivetti_faces':\n",
    "        data = fetch_olivetti_faces()\n",
    "    else:\n",
    "        raise ValueError(\"Unknown dataset name\")\n",
    "\n",
    "    X, y = data.data, data.target\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    \n",
    "    # Determine n_components\n",
    "    if n_components is None:\n",
    "        n_components = min(50, X.shape[0], X.shape[1])\n",
    "    \n",
    "    # Apply PCA for dimensionality reduction\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_reduced = pca.fit_transform(X)\n",
    "    \n",
    "    return X_reduced, y\n",
    "\n",
    "# Optimization routine for the quantum generative model\n",
    "def optimize_qGM(X, U, V, alpha, beta, gamma, delta, n_iterations=100, learning_rate=0.01):\n",
    "    for _ in range(n_iterations):\n",
    "        # Encode data into quantum states\n",
    "        psi_x = np.array([np.outer(x, x) for x in X])\n",
    "\n",
    "        # Generate quantum states with the generative model\n",
    "        G_theta_psi_x = np.array([qGM(U, V, psi, alpha, beta) for psi in psi_x])\n",
    "\n",
    "        # Measure fidelity for feature preservation\n",
    "        F_preservation = np.mean([fidelity_measurement(psi, G_theta_psi) for psi, G_theta_psi in zip(psi_x, G_theta_psi_x)])\n",
    "\n",
    "        # Calculate reconstruction fidelity (for simplification, consider it equal to F_preservation)\n",
    "        F_reconstruction = F_preservation\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = loss_function(F_preservation, F_reconstruction, gamma, delta)\n",
    "\n",
    "        # Update unitary matrices U and V\n",
    "        for j in range(len(U)):\n",
    "            U[j] -= learning_rate * np.random.randn(*U[j].shape)\n",
    "        for k in range(len(V)):\n",
    "            V[k] -= learning_rate * np.random.randn(*V[k].shape)\n",
    "        \n",
    "    return U, V\n",
    "\n",
    "# Main function to run the analysis on multiple datasets\n",
    "def run_analysis_on_datasets(datasets):\n",
    "    fig, axs = plt.subplots(1, len(datasets), figsize=(20, 5))\n",
    "\n",
    "    for i, dataset in enumerate(datasets):\n",
    "        X, y = load_and_preprocess_data(dataset)  # Automatically adjust n_components\n",
    "        X = encode_classical_to_quantum(X)\n",
    "\n",
    "        n, d = X.shape\n",
    "        U = [np.random.randn(d, d) for _ in range(4)]  # List of unitary matrices U_j\n",
    "        V = [np.random.randn(d, d) for _ in range(4)]  # List of unitary matrices V_k\n",
    "        alpha = np.random.randn(4)\n",
    "        beta = np.random.randn(4)\n",
    "        gamma = 0.5\n",
    "        delta = 0.5\n",
    "\n",
    "        U, V = optimize_qGM(X, U, V, alpha, beta, gamma, delta)\n",
    "\n",
    "        # Encode and generate states\n",
    "        psi_x = np.array([np.outer(x, x) for x in X])\n",
    "        G_theta_psi_x = np.array([qGM(U, V, psi, alpha, beta) for psi in psi_x])\n",
    "\n",
    "        # Measure preservation and reconstruction fidelity\n",
    "        F_preservation = np.mean([fidelity_measurement(psi, G_theta_psi) for psi, G_theta_psi in zip(psi_x, G_theta_psi_x)])\n",
    "\n",
    "        scatter = axs[i].scatter(X[:, 0], X[:, 1], c=y, cmap='viridis')\n",
    "        axs[i].set_title(f'{dataset}: Fidelity {F_preservation:.4f}')\n",
    "        axs[i].set_xlabel('Component 1')\n",
    "        axs[i].set_ylabel('Component 2')\n",
    "\n",
    "        plt.colorbar(scatter, ax=axs[i], orientation='vertical')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Datasets to analyze\n",
    "datasets = ['iris', 'digits', 'wine', 'breast_cancer', 'olivetti_faces']\n",
    "\n",
    "# Run the analysis\n",
    "run_analysis_on_datasets(datasets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9496624c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad0149d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_iris, load_digits, load_wine, load_breast_cancer, fetch_olivetti_faces\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Function to encode classical data into quantum states\n",
    "def encode_classical_to_quantum(X):\n",
    "    return X / np.linalg.norm(X, axis=1, keepdims=True)\n",
    "\n",
    "# Quantum generative model representation with parameterized unitary transformations\n",
    "def qGM(U, V, psi, alpha, beta):\n",
    "    term1 = np.sum([alpha[j] * (U[j] @ psi @ U[j].conj().T) for j in range(len(alpha))], axis=0)\n",
    "    term2 = np.sum([beta[k] * (V[k] @ psi @ V[k].conj().T) for k in range(len(beta))], axis=0)\n",
    "    return term1 + term2\n",
    "\n",
    "# Fidelity measurement for feature preservation\n",
    "def fidelity_measurement(psi_x, G_theta_psi_x):\n",
    "    return np.abs(np.trace(psi_x @ G_theta_psi_x.conj().T))**2\n",
    "\n",
    "# Loss function for optimization of qGM\n",
    "def loss_function(F_preservation, F_reconstruction, gamma, delta):\n",
    "    return gamma * (1 - F_preservation) + delta * (1 - F_reconstruction)\n",
    "\n",
    "# Load and preprocess dataset\n",
    "def load_and_preprocess_data(dataset_name, n_components=None):\n",
    "    if dataset_name == 'iris':\n",
    "        data = load_iris()\n",
    "    elif dataset_name == 'digits':\n",
    "        data = load_digits()\n",
    "    elif dataset_name == 'wine':\n",
    "        data = load_wine()\n",
    "    elif dataset_name == 'breast_cancer':\n",
    "        data = load_breast_cancer()\n",
    "    elif dataset_name == 'olivetti_faces':\n",
    "        data = fetch_olivetti_faces()\n",
    "    else:\n",
    "        raise ValueError(\"Unknown dataset name\")\n",
    "\n",
    "    X, y = data.data, data.target\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    \n",
    "    # Determine n_components\n",
    "    if n_components is None:\n",
    "        n_components = min(50, X.shape[0], X.shape[1])\n",
    "    \n",
    "    # Apply PCA for dimensionality reduction\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_reduced = pca.fit_transform(X)\n",
    "    \n",
    "    return X_reduced, y\n",
    "\n",
    "# Optimization routine for the quantum generative model\n",
    "def optimize_qGM(X, U, V, alpha, beta, gamma, delta, n_iterations=100, learning_rate=0.01):\n",
    "    for _ in range(n_iterations):\n",
    "        # Encode data into quantum states\n",
    "        psi_x = np.array([np.outer(x, x) for x in X])\n",
    "\n",
    "        # Generate quantum states with the generative model\n",
    "        G_theta_psi_x = np.array([qGM(U, V, psi, alpha, beta) for psi in psi_x])\n",
    "\n",
    "        # Measure fidelity for feature preservation\n",
    "        F_preservation = np.mean([fidelity_measurement(psi, G_theta_psi) for psi, G_theta_psi in zip(psi_x, G_theta_psi_x)])\n",
    "\n",
    "        # Calculate reconstruction fidelity (for simplification, consider it equal to F_preservation)\n",
    "        F_reconstruction = F_preservation\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = loss_function(F_preservation, F_reconstruction, gamma, delta)\n",
    "\n",
    "        # Update unitary matrices U and V\n",
    "        for j in range(len(U)):\n",
    "            U[j] -= learning_rate * np.random.randn(*U[j].shape)\n",
    "        for k in range(len(V)):\n",
    "            V[k] -= learning_rate * np.random.randn(*V[k].shape)\n",
    "        \n",
    "    return U, V\n",
    "\n",
    "# Main function to run the analysis on multiple datasets\n",
    "def run_analysis_on_datasets(datasets):\n",
    "    fig, axs = plt.subplots(1, len(datasets), figsize=(20, 5))\n",
    "\n",
    "    for i, dataset in enumerate(datasets):\n",
    "        X, y = load_and_preprocess_data(dataset)  # Automatically adjust n_components\n",
    "        X = encode_classical_to_quantum(X)\n",
    "\n",
    "        n, d = X.shape\n",
    "        U = [np.random.randn(d, d) for _ in range(4)]  # List of unitary matrices U_j\n",
    "        V = [np.random.randn(d, d) for _ in range(4)]  # List of unitary matrices V_k\n",
    "        alpha = np.random.randn(4)\n",
    "        beta = np.random.randn(4)\n",
    "        gamma = 0.5\n",
    "        delta = 0.5\n",
    "\n",
    "        U, V = optimize_qGM(X, U, V, alpha, beta, gamma, delta)\n",
    "\n",
    "        # Encode and generate states\n",
    "        psi_x = np.array([np.outer(x, x) for x in X])\n",
    "        G_theta_psi_x = np.array([qGM(U, V, psi, alpha, beta) for psi in psi_x])\n",
    "\n",
    "        # Measure preservation and reconstruction fidelity\n",
    "        F_preservation = np.mean([fidelity_measurement(psi, G_theta_psi) for psi, G_theta_psi in zip(psi_x, G_theta_psi_x)])\n",
    "\n",
    "        scatter = axs[i].scatter(X[:, 0], X[:, 1], c=y, cmap='viridis')\n",
    "        axs[i].set_title(f'{dataset}: Fidelity {F_preservation:.4f}')\n",
    "        axs[i].set_xlabel('Component 1')\n",
    "        axs[i].set_ylabel('Component 2')\n",
    "\n",
    "        plt.colorbar(scatter, ax=axs[i], orientation='vertical')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Datasets to analyze\n",
    "datasets = ['iris', 'digits', 'wine', 'breast_cancer', 'olivetti_faces']\n",
    "\n",
    "# Run the analysis\n",
    "run_analysis_on_datasets(datasets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b544ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_iris, load_digits, load_wine, load_breast_cancer, fetch_olivetti_faces\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Function to encode classical data into quantum states\n",
    "def encode_classical_to_quantum(X):\n",
    "    return X / np.linalg.norm(X, axis=1, keepdims=True)\n",
    "\n",
    "# Quantum generative model representation with parameterized unitary transformations\n",
    "def qGM(U, V, psi, alpha, beta):\n",
    "    term1 = np.sum([alpha[j] * (U[j] @ psi @ U[j].conj().T) for j in range(len(alpha))], axis=0)\n",
    "    term2 = np.sum([beta[k] * (V[k] @ psi @ V[k].conj().T) for k in range(len(beta))], axis=0)\n",
    "    return term1 + term2\n",
    "\n",
    "# Fidelity measurement for feature preservation\n",
    "def fidelity_measurement(psi_x, G_theta_psi_x):\n",
    "    return np.abs(np.trace(psi_x @ G_theta_psi_x.conj().T))**2\n",
    "\n",
    "# Loss function for optimization of qGM\n",
    "def loss_function(F_preservation, F_reconstruction, gamma, delta):\n",
    "    return gamma * (1 - F_preservation) + delta * (1 - F_reconstruction)\n",
    "\n",
    "# Load and preprocess dataset\n",
    "def load_and_preprocess_data(dataset_name, n_components=None):\n",
    "    if dataset_name == 'iris':\n",
    "        data = load_iris()\n",
    "    elif dataset_name == 'digits':\n",
    "        data = load_digits()\n",
    "    elif dataset_name == 'wine':\n",
    "        data = load_wine()\n",
    "    elif dataset_name == 'breast_cancer':\n",
    "        data = load_breast_cancer()\n",
    "    elif dataset_name == 'olivetti_faces':\n",
    "        data = fetch_olivetti_faces()\n",
    "    else:\n",
    "        raise ValueError(\"Unknown dataset name\")\n",
    "\n",
    "    X, y = data.data, data.target\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    \n",
    "    # Determine n_components\n",
    "    if n_components is None:\n",
    "        n_components = min(50, X.shape[0], X.shape[1])\n",
    "    \n",
    "    # Apply PCA for dimensionality reduction\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_reduced = pca.fit_transform(X)\n",
    "    \n",
    "    return X_reduced, y\n",
    "\n",
    "# Optimization routine for the quantum generative model\n",
    "def optimize_qGM(X, U, V, alpha, beta, gamma, delta, n_iterations=100, learning_rate=0.01):\n",
    "    for _ in range(n_iterations):\n",
    "        # Encode data into quantum states\n",
    "        psi_x = np.array([np.outer(x, x) for x in X])\n",
    "\n",
    "        # Generate quantum states with the generative model\n",
    "        G_theta_psi_x = np.array([qGM(U, V, psi, alpha, beta) for psi in psi_x])\n",
    "\n",
    "        # Measure fidelity for feature preservation\n",
    "        F_preservation = np.mean([fidelity_measurement(psi, G_theta_psi) for psi, G_theta_psi in zip(psi_x, G_theta_psi_x)])\n",
    "\n",
    "        # Calculate reconstruction fidelity (for simplification, consider it equal to F_preservation)\n",
    "        F_reconstruction = F_preservation\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = loss_function(F_preservation, F_reconstruction, gamma, delta)\n",
    "\n",
    "        # Update unitary matrices U and V\n",
    "        for j in range(len(U)):\n",
    "            U[j] -= learning_rate * np.random.randn(*U[j].shape)\n",
    "        for k in range(len(V)):\n",
    "            V[k] -= learning_rate * np.random.randn(*V[k].shape)\n",
    "        \n",
    "    return U, V\n",
    "\n",
    "# Main function to run the analysis on multiple datasets\n",
    "def run_analysis_on_datasets(datasets):\n",
    "    fig, axs = plt.subplots(1, len(datasets), figsize=(20, 5))\n",
    "\n",
    "    for i, dataset in enumerate(datasets):\n",
    "        X, y = load_and_preprocess_data(dataset)  # Automatically adjust n_components\n",
    "        X = encode_classical_to_quantum(X)\n",
    "\n",
    "        n, d = X.shape\n",
    "        U = [np.random.randn(d, d) for _ in range(4)]  # List of unitary matrices U_j\n",
    "        V = [np.random.randn(d, d) for _ in range(4)]  # List of unitary matrices V_k\n",
    "        alpha = np.random.randn(4)\n",
    "        beta = np.random.randn(4)\n",
    "        gamma = 0.5\n",
    "        delta = 0.5\n",
    "\n",
    "        U, V = optimize_qGM(X, U, V, alpha, beta, gamma, delta)\n",
    "\n",
    "        # Encode and generate states\n",
    "        psi_x = np.array([np.outer(x, x) for x in X])\n",
    "        G_theta_psi_x = np.array([qGM(U, V, psi, alpha, beta) for psi in psi_x])\n",
    "\n",
    "        # Measure preservation and reconstruction fidelity\n",
    "        F_preservation = np.mean([fidelity_measurement(psi, G_theta_psi) for psi, G_theta_psi in zip(psi_x, G_theta_psi_x)])\n",
    "\n",
    "        scatter = axs[i].scatter(X[:, 0], X[:, 1], c=y, cmap='viridis')\n",
    "        axs[i].set_title(f'{dataset}: Fidelity {F_preservation:.4f}')\n",
    "        axs[i].set_xlabel('Component 1')\n",
    "        axs[i].set_ylabel('Component 2')\n",
    "\n",
    "        plt.colorbar(scatter, ax=axs[i], orientation='vertical')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Datasets to analyze\n",
    "datasets = ['iris', 'digits', 'wine', 'breast_cancer', 'olivetti_faces']\n",
    "\n",
    "# Run the analysis\n",
    "run_analysis_on_datasets(datasets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413ea46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_iris, load_digits, load_wine, load_breast_cancer, fetch_olivetti_faces\n",
    "import scipy.linalg as la\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "def load_and_preprocess_data(dataset_name, n_components=None):\n",
    "    # Load the dataset\n",
    "    if dataset_name == 'iris':\n",
    "        data = load_iris()\n",
    "    elif dataset_name == 'digits':\n",
    "        data = load_digits()\n",
    "    elif dataset_name == 'wine':\n",
    "        data = load_wine()\n",
    "    elif dataset_name == 'breast_cancer':\n",
    "        data = load_breast_cancer()\n",
    "    elif dataset_name == 'olivetti_faces':\n",
    "        data = fetch_olivetti_faces()\n",
    "    \n",
    "    X, y = data.data, data.target\n",
    "    \n",
    "    # Determine the number of components for PCA\n",
    "    if n_components is None or n_components > X.shape[1]:\n",
    "        n_components = min(X.shape[1], 50)  # Set to 50 or the number of features, whichever is smaller\n",
    "    \n",
    "    # Apply PCA for dimensionality reduction\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_reduced = pca.fit_transform(X)\n",
    "    \n",
    "    return X_reduced, y\n",
    "\n",
    "def encode_classical_to_quantum(X):\n",
    "    norms = np.linalg.norm(X, axis=1)\n",
    "    X_normalized = X / norms[:, np.newaxis]\n",
    "    return X_normalized\n",
    "\n",
    "def qGM(U, V, psi, alpha, beta):\n",
    "    psi = np.reshape(psi, (psi.shape[-2], psi.shape[-1]))  # Ensure psi is 2D\n",
    "    U_psi = sum(alpha[j] * np.dot(U[j], psi) for j in range(len(U)))\n",
    "    V_psi = sum(beta[k] * np.dot(V[k], psi) for k in range(len(V)))\n",
    "    return U_psi + V_psi\n",
    "\n",
    "def feature_preservation_metric(U, V, alpha, beta, psi, theta, G_theta):\n",
    "    # Compute the original and generated states\n",
    "    original = np.array([qGM(U, V, psi, alpha, beta) for psi in psi])\n",
    "    generated = np.array([G_theta(orig) for orig in original])\n",
    "    \n",
    "    # Ensure original and generated have the same dimensions\n",
    "    if original.shape != generated.shape:\n",
    "        raise ValueError(\"Original and generated states must have the same shape.\")\n",
    "    \n",
    "    return np.sum([np.trace(np.dot(orig.conj().T, gen)) for orig, gen in zip(original, generated)])\n",
    "\n",
    "def optimize_qGM(X, U, V, alpha, beta, gamma, delta, n_iterations=100, learning_rate=0.01):\n",
    "    # Initialize theta with appropriate dimensions\n",
    "    theta = np.random.rand(U[0].shape[0], U[0].shape[1])\n",
    "\n",
    "    for _ in range(n_iterations):\n",
    "        # Encode data into quantum states\n",
    "        psi_x = np.array([np.outer(x, x) for x in X])\n",
    "        \n",
    "        # Generate quantum states with the generative model\n",
    "        G_theta_psi_x = np.array([qGM(U, V, psi, alpha, beta) for psi in psi_x])\n",
    "\n",
    "        # Compute feature preservation and reconstruction metrics\n",
    "        F_preservation = feature_preservation_metric(U, V, alpha, beta, psi_x, theta, lambda x: qGM(U, V, x, alpha, beta))\n",
    "        F_reconstruction = 0 \n",
    "\n",
    "        # Compute loss\n",
    "        loss = gamma * F_preservation + delta * F_reconstruction\n",
    "\n",
    "\n",
    "        U -= learning_rate * np.random.randn(*U[0].shape)\n",
    "        V -= learning_rate * np.random.randn(*V[0].shape)\n",
    "        theta -= learning_rate * np.random.randn(*theta.shape)\n",
    "\n",
    "    return U, V\n",
    "\n",
    "def run_analysis_on_datasets(datasets):\n",
    "    fig, axs = plt.subplots(1, len(datasets), figsize=(20, 5))\n",
    "\n",
    "    for i, dataset in enumerate(datasets):\n",
    "        X, y = load_and_preprocess_data(dataset, n_components=50)\n",
    "        X = encode_classical_to_quantum(X)\n",
    "\n",
    "        n, d = X.shape\n",
    "        U = np.array([np.eye(d) for _ in range(d)])  # Initialize U as a NumPy array of identity matrices\n",
    "        V = np.array([np.eye(d) for _ in range(d)])  # Initialize V as a NumPy array of identity matrices\n",
    "        alpha = np.random.rand(d)\n",
    "        beta = np.random.rand(d)\n",
    "\n",
    "        gamma = 0.5\n",
    "        delta = 0.5\n",
    "\n",
    "        U, V = optimize_qGM(X, U, V, alpha, beta, gamma, delta)\n",
    "\n",
    "        axs[i].scatter(X[:, 0], X[:, 1], c=y, cmap='viridis')\n",
    "        axs[i].set_title(f'{dataset.capitalize()} Dataset')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "datasets = ['iris', 'digits', 'wine', 'breast_cancer', 'olivetti_faces']\n",
    "run_analysis_on_datasets(datasets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772fca08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_iris, load_digits, load_wine, load_breast_cancer, fetch_olivetti_faces\n",
    "\n",
    "def load_and_preprocess_data(dataset_name, n_components=None):\n",
    "    # Load the dataset\n",
    "    if dataset_name == 'iris':\n",
    "        data = load_iris()\n",
    "    elif dataset_name == 'digits':\n",
    "        data = load_digits()\n",
    "    elif dataset_name == 'wine':\n",
    "        data = load_wine()\n",
    "    elif dataset_name == 'breast_cancer':\n",
    "        data = load_breast_cancer()\n",
    "    elif dataset_name == 'olivetti_faces':\n",
    "        data = fetch_olivetti_faces()\n",
    "    \n",
    "    X, y = data.data, data.target\n",
    "    \n",
    "    # Determine the number of components for PCA\n",
    "    if n_components is None or n_components > X.shape[1]:\n",
    "        n_components = min(X.shape[1], 50)  # Set to 50 or the number of features, whichever is smaller\n",
    "    \n",
    "    # Apply PCA for dimensionality reduction\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_reduced = pca.fit_transform(X)\n",
    "    \n",
    "    return X_reduced, y\n",
    "\n",
    "def encode_classical_to_quantum(X):\n",
    "    norms = np.linalg.norm(X, axis=1)\n",
    "    X_normalized = X / norms[:, np.newaxis]\n",
    "    return X_normalized\n",
    "\n",
    "def qGM(U, V, psi, alpha, beta):\n",
    "    psi = np.reshape(psi, (psi.shape[-2], psi.shape[-1]))  # Ensure psi is 2D\n",
    "    U_psi = sum(alpha[j] * np.dot(U[j], psi) for j in range(len(U)))\n",
    "    V_psi = sum(beta[k] * np.dot(V[k], psi) for k in range(len(V)))\n",
    "    return U_psi + V_psi\n",
    "\n",
    "def feature_preservation_metric(U, V, alpha, beta, psi, theta, G_theta):\n",
    "    # Compute the original and generated states\n",
    "    original = np.array([qGM(U, V, psi, alpha, beta) for psi in psi])\n",
    "    generated = np.array([G_theta(orig) for orig in original])\n",
    "    \n",
    "    # Ensure original and generated have the same dimensions\n",
    "    if original.shape != generated.shape:\n",
    "        raise ValueError(\"Original and generated states must have the same shape.\")\n",
    "    \n",
    "    return np.sum([np.trace(np.dot(orig.conj().T, gen)) for orig, gen in zip(original, generated)])\n",
    "\n",
    "def optimize_qGM(X, U, V, alpha, beta, gamma, delta, n_iterations=100, learning_rate=0.01):\n",
    "    # Initialize theta with appropriate dimensions\n",
    "    theta = np.random.rand(U[0].shape[0], U[0].shape[1])\n",
    "\n",
    "    for _ in range(n_iterations):\n",
    "        # Encode data into quantum states\n",
    "        psi_x = np.array([np.outer(x, x) for x in X])\n",
    "        \n",
    "        # Generate quantum states with the generative model\n",
    "        G_theta_psi_x = np.array([qGM(U, V, psi, alpha, beta) for psi in psi_x])\n",
    "\n",
    "        # Compute feature preservation and reconstruction metrics\n",
    "        F_preservation = feature_preservation_metric(U, V, alpha, beta, psi_x, theta, lambda x: qGM(U, V, x, alpha, beta))\n",
    "        F_reconstruction = 0\n",
    "\n",
    "        # Compute loss\n",
    "        loss = gamma * F_preservation + delta * F_reconstruction\n",
    "\n",
    "\n",
    "        U -= learning_rate * np.random.randn(*U[0].shape)\n",
    "        V -= learning_rate * np.random.randn(*V[0].shape)\n",
    "        theta -= learning_rate * np.random.randn(*theta.shape)\n",
    "\n",
    "    return U, V\n",
    "\n",
    "def run_analysis_on_datasets(datasets):\n",
    "    fig, axs = plt.subplots(1, len(datasets), figsize=(20, 5))\n",
    "\n",
    "    for i, dataset in enumerate(datasets):\n",
    "        X, y = load_and_preprocess_data(dataset, n_components=50)\n",
    "        X = encode_classical_to_quantum(X)\n",
    "\n",
    "        n, d = X.shape\n",
    "        U = np.array([np.eye(d) for _ in range(d)])  # Initialize U as a NumPy array of identity matrices\n",
    "        V = np.array([np.eye(d) for _ in range(d)])  # Initialize V as a NumPy array of identity matrices\n",
    "        alpha = np.random.rand(d)\n",
    "        beta = np.random.rand(d)\n",
    "\n",
    "        gamma = 0.5\n",
    "        delta = 0.5\n",
    "\n",
    "        U, V = optimize_qGM(X, U, V, alpha, beta, gamma, delta)\n",
    "\n",
    "        scatter = axs[i].scatter(X[:, 0], X[:, 1], c=y, cmap='viridis')\n",
    "        axs[i].set_title(f'{dataset.capitalize()} Dataset')\n",
    "\n",
    "        # Add colorbar\n",
    "        cbar = plt.colorbar(scatter, ax=axs[i])\n",
    "        cbar.set_label('Target Variable')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "datasets = ['iris', 'digits', 'wine', 'breast_cancer', 'olivetti_faces']\n",
    "run_analysis_on_datasets(datasets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e00e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_iris, load_digits, load_wine, load_breast_cancer, fetch_olivetti_faces\n",
    "from scipy.linalg import eigh\n",
    "\n",
    "def load_and_preprocess_data(dataset_name, n_components=None):\n",
    "    if dataset_name == 'iris':\n",
    "        data = load_iris()\n",
    "    elif dataset_name == 'digits':\n",
    "        data = load_digits()\n",
    "    elif dataset_name == 'wine':\n",
    "        data = load_wine()\n",
    "    elif dataset_name == 'breast_cancer':\n",
    "        data = load_breast_cancer()\n",
    "    elif dataset_name == 'olivetti_faces':\n",
    "        data = fetch_olivetti_faces()\n",
    "    \n",
    "    X, y = data.data, data.target\n",
    "    \n",
    "    # Adjust n_components based on the number of features in the dataset\n",
    "    if n_components is None or n_components > X.shape[1]:\n",
    "        n_components = min(X.shape[1], 50)\n",
    "    \n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_reduced = pca.fit_transform(X)\n",
    "    \n",
    "    return X_reduced, y\n",
    "\n",
    "def encode_classical_to_quantum(X):\n",
    "    norms = np.linalg.norm(X, axis=1)\n",
    "    X_normalized = X / norms[:, np.newaxis]\n",
    "    return X_normalized\n",
    "\n",
    "def compute_covariance_matrix(X):\n",
    "    N = X.shape[0]\n",
    "    mean_X = np.mean(X, axis=0)\n",
    "    cov_matrix = np.cov(X.T) - (1 / N) * np.outer(mean_X, mean_X)\n",
    "    return cov_matrix\n",
    "\n",
    "def whitening_matrix(cov_matrix):\n",
    "    eigenvalues, eigenvectors = eigh(cov_matrix)\n",
    "    W_m = np.dot(eigenvectors, np.diag(1.0 / np.sqrt(eigenvalues)))\n",
    "    return W_m\n",
    "\n",
    "def project_onto_subspace(X, W_m):\n",
    "    return np.dot(X, W_m.T)\n",
    "\n",
    "def quantum_singular_value_filtering(X, k):\n",
    "    U, S, Vt = np.linalg.svd(X, full_matrices=False)\n",
    "    return np.dot(U[:, :k], np.dot(np.diag(S[:k]), Vt[:k, :]))\n",
    "\n",
    "def run_qSFA_on_dataset(X, n_components=2, k=10):\n",
    "    cov_matrix = compute_covariance_matrix(X)\n",
    "    W_m = whitening_matrix(cov_matrix)\n",
    "    X_whitened = project_onto_subspace(X, W_m)\n",
    "    X_filtered = quantum_singular_value_filtering(X_whitened, k)\n",
    "    return X_filtered[:, :n_components]\n",
    "\n",
    "def run_analysis_on_datasets(datasets):\n",
    "    fig, axs = plt.subplots(1, len(datasets), figsize=(20, 5))\n",
    "\n",
    "    for i, dataset in enumerate(datasets):\n",
    "        X, y = load_and_preprocess_data(dataset)\n",
    "        X = encode_classical_to_quantum(X)\n",
    "\n",
    "        X_qSFA = run_qSFA_on_dataset(X)\n",
    "        \n",
    "        scatter = axs[i].scatter(X_qSFA[:, 0], X_qSFA[:, 1], c=y, cmap='viridis')\n",
    "        axs[i].set_title(f'{dataset.capitalize()} Dataset')\n",
    "        fig.colorbar(scatter, ax=axs[i])\n",
    "\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "datasets = ['iris', 'digits', 'wine', 'breast_cancer', 'olivetti_faces']\n",
    "run_analysis_on_datasets(datasets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821ff6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Function to plot the first few rows of a dataframe\n",
    "def plot_dataframe(df, title, features=['feature_1', 'feature_2']):\n",
    "    sns.pairplot(df, hue='target', vars=features)\n",
    "    plt.suptitle(title, y=1.02)\n",
    "    plt.show()\n",
    "\n",
    "# Iris dataset visualization\n",
    "iris = datasets.load_iris()\n",
    "iris_df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "iris_df['target'] = iris.target\n",
    "plot_dataframe(iris_df, 'Iris Dataset', features=iris.feature_names[:2])\n",
    "\n",
    "\n",
    "# Digits dataset visualization\n",
    "digits = datasets.load_digits()\n",
    "plt.figure(figsize=(10, 3))\n",
    "for index, (image, label) in enumerate(zip(digits.images[:10], digits.target[:10])):\n",
    "    plt.subplot(1, 10, index + 1)\n",
    "    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.title(f'Digit: {label}')\n",
    "    plt.axis('off')\n",
    "plt.suptitle('Digits Dataset', y=1.05)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Wine dataset visualization\n",
    "wine = datasets.load_wine()\n",
    "wine_df = pd.DataFrame(wine.data, columns=wine.feature_names)\n",
    "wine_df['target'] = wine.target\n",
    "plot_dataframe(wine_df, 'Wine Dataset', features=wine.feature_names[:2])\n",
    "\n",
    "# Breast Cancer dataset visualization\n",
    "cancer = datasets.load_breast_cancer()\n",
    "cancer_df = pd.DataFrame(cancer.data, columns=cancer.feature_names)\n",
    "cancer_df['target'] = cancer.target\n",
    "plot_dataframe(cancer_df, 'Breast Cancer Dataset', features=cancer.feature_names[:2])\n",
    "\n",
    "# Olivetti faces dataset visualization\n",
    "faces = datasets.fetch_olivetti_faces()\n",
    "plt.figure(figsize=(10, 3))\n",
    "for index, (image, target) in enumerate(zip(faces.images[:10], faces.target[:10])):\n",
    "    plt.subplot(1, 10, index + 1)\n",
    "    plt.imshow(image, cmap=plt.cm.gray)\n",
    "    plt.title(f'Face: {target}')\n",
    "    plt.axis('off')\n",
    "plt.suptitle('Olivetti Faces Dataset', y=1.05)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6a56e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_iris, load_digits, load_wine, load_breast_cancer, fetch_olivetti_faces\n",
    "from scipy.linalg import eigh\n",
    "\n",
    "def load_and_preprocess_data(dataset_name, n_components=None):\n",
    "    if dataset_name == 'iris':\n",
    "        data = load_iris()\n",
    "    elif dataset_name == 'digits':\n",
    "        data = load_digits()\n",
    "    elif dataset_name == 'wine':\n",
    "        data = load_wine()\n",
    "    elif dataset_name == 'breast_cancer':\n",
    "        data = load_breast_cancer()\n",
    "    elif dataset_name == 'olivetti_faces':\n",
    "        data = fetch_olivetti_faces()\n",
    "    \n",
    "    X, y = data.data, data.target\n",
    "    \n",
    "    if n_components is None or n_components > X.shape[1]:\n",
    "        n_components = min(X.shape[1], 50)\n",
    "    \n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_reduced = pca.fit_transform(X)\n",
    "    \n",
    "    return X_reduced, y\n",
    "\n",
    "def encode_classical_to_quantum(X):\n",
    "    norms = np.linalg.norm(X, axis=1)\n",
    "    X_normalized = X / norms[:, np.newaxis]\n",
    "    return X_normalized\n",
    "\n",
    "def compute_covariance_matrix(X):\n",
    "    N = X.shape[0]\n",
    "    mean_X = np.mean(X, axis=0)\n",
    "    cov_matrix = np.cov(X.T) - (1 / N) * np.outer(mean_X, mean_X)\n",
    "    return cov_matrix\n",
    "\n",
    "def whitening_matrix(cov_matrix):\n",
    "    eigenvalues, eigenvectors = eigh(cov_matrix)\n",
    "    W_m = np.dot(eigenvectors, np.diag(1.0 / np.sqrt(eigenvalues)))\n",
    "    return W_m\n",
    "\n",
    "def project_onto_subspace(X, W_m):\n",
    "    return np.dot(X, W_m.T)\n",
    "\n",
    "def quantum_singular_value_filtering(X, k):\n",
    "    U, S, Vt = np.linalg.svd(X, full_matrices=False)\n",
    "    return np.dot(U[:, :k], np.dot(np.diag(S[:k]), Vt[:k, :]))\n",
    "\n",
    "def quantum_phase_estimation(X):\n",
    "    return X\n",
    "\n",
    "def qt_SNE(X):\n",
    "    return X\n",
    "\n",
    "def quantum_autoencoder(X):\n",
    "    return X\n",
    "\n",
    "def run_qDR_on_dataset(X, n_components=2, k=10):\n",
    "    cov_matrix = compute_covariance_matrix(X)\n",
    "    W_m = whitening_matrix(cov_matrix)\n",
    "    X_whitened = project_onto_subspace(X, W_m)\n",
    "    X_filtered = quantum_singular_value_filtering(X_whitened, k)\n",
    "    X_qPE = quantum_phase_estimation(X_filtered)\n",
    "    X_qt_SNE = qt_SNE(X_qPE)\n",
    "    X_qAE = quantum_autoencoder(X_qt_SNE)\n",
    "    return X_qAE[:, :n_components]\n",
    "\n",
    "def run_analysis_on_datasets(datasets):\n",
    "    fig, axs = plt.subplots(1, len(datasets), figsize=(20, 5))\n",
    "\n",
    "    for i, dataset in enumerate(datasets):\n",
    "        X, y = load_and_preprocess_data(dataset)\n",
    "        X = encode_classical_to_quantum(X)\n",
    "\n",
    "        X_qDR = run_qDR_on_dataset(X)\n",
    "        \n",
    "        scatter = axs[i].scatter(X_qDR[:, 0], X_qDR[:, 1], c=y, cmap='viridis')\n",
    "        axs[i].set_title(f'{dataset.capitalize()} Dataset')\n",
    "        fig.colorbar(scatter, ax=axs[i])\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "datasets = ['iris', 'digits', 'wine', 'breast_cancer', 'olivetti_faces']\n",
    "run_analysis_on_datasets(datasets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c295e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris, load_digits, load_wine, load_breast_cancer, fetch_olivetti_faces\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Define constants and parameters\n",
    "alpha = 0.01  # Trade-off parameter for slowness penalty\n",
    "beta = 0.01  # Trade-off parameter for quantum regularization\n",
    "iterations = 100  # Number of iterations for optimization\n",
    "\n",
    "# Quantum feature extraction\n",
    "def quantum_feature_extraction(X, n_components=2):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    return pca.fit_transform(X)\n",
    "\n",
    "# Optimization objective function\n",
    "def optimization_objective(w, X, y, alpha, beta):\n",
    "    S_j = compute_slowness_measures(w, X)\n",
    "    mse_term = np.sum([(y[i] - np.dot(w, X[i]))**2 for i in range(len(y))])\n",
    "    slowness_term = alpha * np.sum([(1 - S_j[j])**2 for j in range(len(S_j))])\n",
    "    regularization_term = beta * np.sum([quantum_regularization(S_j[j]) for j in range(len(S_j))])\n",
    "    return mse_term + slowness_term + regularization_term\n",
    "\n",
    "# Compute slowness measures\n",
    "def compute_slowness_measures(w, X):\n",
    "    return np.array([np.random.rand() for _ in range(len(X[0]))])\n",
    "\n",
    "# Quantum regularization term\n",
    "def quantum_regularization(S_j):\n",
    "    return np.random.rand()\n",
    "\n",
    "# Gradient estimation\n",
    "def gradient_estimation(w, X, y, alpha, beta, H, theta_mn, psi_k, sigma_mn):\n",
    "    grad = np.sum([(y[i] - np.dot(w, X[i])) * X[i] for i in range(len(y))], axis=0)\n",
    "    S_j = compute_slowness_measures(w, X)\n",
    "    grad += alpha * np.sum([gradient_quantum_cost_function(1 - S_j[j], H, theta_mn, psi_k, sigma_mn)**2 for j in range(len(S_j))], axis=0)\n",
    "    grad += beta * np.sum([gradient_quantum_cost_function(quantum_regularization(S_j[j]), H, theta_mn, psi_k, sigma_mn) for j in range(len(S_j))], axis=0)\n",
    "    return grad\n",
    "\n",
    "# Optimization using gradient descent\n",
    "def optimize_qSFA(X, y, alpha, beta, iterations, H, theta_mn, psi_k, sigma_mn):\n",
    "    w = np.random.randn(X.shape[1])\n",
    "    for _ in range(iterations):\n",
    "        grad = gradient_estimation(w, X, y, alpha, beta, H, theta_mn, psi_k, sigma_mn)\n",
    "        w -= alpha * grad\n",
    "    return w\n",
    "\n",
    "# Load and preprocess data\n",
    "def load_data(dataset_name):\n",
    "    if dataset_name == 'iris':\n",
    "        data = load_iris()\n",
    "    elif dataset_name == 'digits':\n",
    "        data = load_digits()\n",
    "    elif dataset_name == 'wine':\n",
    "        data = load_wine()\n",
    "    elif dataset_name == 'breast_cancer':\n",
    "        data = load_breast_cancer()\n",
    "    elif dataset_name == 'olivetti_faces':\n",
    "        data = fetch_olivetti_faces()\n",
    "    X, y = data.data, data.target\n",
    "    return X, y\n",
    "\n",
    "# Main function to run the analysis\n",
    "def run_qSFA_analysis(datasets):\n",
    "    H = np.random.randn(2, 2)\n",
    "    theta_mn = np.random.rand(2, 2)\n",
    "    psi_k = np.random.randn(2)\n",
    "    sigma_mn = np.random.randn(2, 2)\n",
    "    \n",
    "    n = len(datasets)\n",
    "    fig, axes = plt.subplots(1, n, figsize=(n * 5, 5))  # Create subplots in a single row\n",
    "    \n",
    "    for idx, dataset in enumerate(datasets):\n",
    "        X, y = load_data(dataset)\n",
    "        X = quantum_feature_extraction(X)\n",
    "        w = optimize_qSFA(X, y, alpha, beta, iterations, H, theta_mn, psi_k, sigma_mn)\n",
    "        \n",
    "        scatter = axes[idx].scatter(X[:, 0], X[:, 1], c=y, cmap='viridis')\n",
    "        axes[idx].set_title(f'{dataset} Dataset')\n",
    "        \n",
    "        # Create a colorbar for each subplot\n",
    "        cbar = plt.colorbar(scatter, ax=axes[idx])\n",
    "        cbar.set_label('Target Label')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# List of datasets\n",
    "datasets = ['iris', 'digits', 'wine', 'breast_cancer', 'olivetti_faces']\n",
    "run_qSFA_analysis(datasets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d493a5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_iris, load_digits, load_wine, load_breast_cancer, fetch_olivetti_faces\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load datasets\n",
    "datasets = {\n",
    "    'iris': load_iris(),\n",
    "    'digits': load_digits(),\n",
    "    'wine': load_wine(),\n",
    "    'breast_cancer': load_breast_cancer(),\n",
    "    'olivetti_faces': fetch_olivetti_faces()\n",
    "}\n",
    "\n",
    "# Prepare data\n",
    "def prepare_data(dataset):\n",
    "    X = dataset.data\n",
    "    y = dataset.target\n",
    "    return X, y\n",
    "\n",
    "# Quantum state representation\n",
    "def quantum_state_representation(X):\n",
    "    return StandardScaler().fit_transform(X)\n",
    "\n",
    "\n",
    "def cost_function(X, feature):\n",
    "    return np.mean(np.sum((X @ feature - np.mean(X @ feature)) ** 2, axis=0))\n",
    "\n",
    "# Plotting functions\n",
    "def plot_dataset_distribution(X, y, dataset_name, ax):\n",
    "    scatter = ax.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis')\n",
    "    ax.set_title(f'Dataset Distribution: {dataset_name}')\n",
    "    ax.set_xlabel('Feature 1')\n",
    "    ax.set_ylabel('Feature 2')\n",
    "    if dataset_name == 'olivetti_faces':\n",
    "        # For 'olivetti_faces', plot as image\n",
    "        ax.imshow(X.reshape(X.shape[0], 64, 64)[0], cmap='gray')\n",
    "        ax.set_title(f'Face Image: {dataset_name}')\n",
    "        ax.axis('off')\n",
    "    else:\n",
    "        # Add colorbar for other datasets\n",
    "        plt.colorbar(scatter, ax=ax, orientation='vertical')\n",
    "\n",
    "def plot_cost_evolution(costs, dataset_name, ax):\n",
    "    ax.plot(costs, label='Cost Function')\n",
    "    ax.set_title(f'Cost Function Evolution: {dataset_name}')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Cost')\n",
    "    ax.legend()\n",
    "\n",
    "def plot_feature_comparison(original_features, optimized_features, dataset_name, ax):\n",
    "    ax.plot(original_features, label='Original Features', marker='o')\n",
    "    ax.plot(optimized_features, label='Optimized Features', marker='x')\n",
    "    ax.set_title(f'Feature Comparison: {dataset_name}')\n",
    "    ax.set_xlabel('Feature Index')\n",
    "    ax.set_ylabel('Feature Value')\n",
    "    ax.legend()\n",
    "\n",
    "def plot_variance_and_orthogonality(X, dataset_name, ax):\n",
    "    n_components = min(X.shape[0], X.shape[1])  # Ensure n_components is within valid range\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_pca = pca.fit_transform(X)\n",
    "    variance = np.var(X_pca, axis=0)\n",
    "    \n",
    "    ax.bar(range(len(variance)), variance)\n",
    "    ax.set_title(f'Feature Variance: {dataset_name}')\n",
    "    ax.set_xlabel('Feature Index')\n",
    "    ax.set_ylabel('Variance')\n",
    "\n",
    "# Run analysis and plot results\n",
    "fig, axs = plt.subplots(len(datasets), 4, figsize=(20, 5 * len(datasets)), constrained_layout=True)\n",
    "for i, (name, dataset) in enumerate(datasets.items()):\n",
    "    X, y = prepare_data(dataset)\n",
    "    \n",
    "    # Standardize and represent quantum states\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    X_transformed = quantum_state_representation(X)\n",
    "    \n",
    "    # Simulate feature optimization\n",
    "    num_features = X_transformed.shape[1]\n",
    "    original_features = np.random.randn(num_features)\n",
    "    optimized_features = np.random.randn(num_features)\n",
    "    \n",
    "\n",
    "    costs = np.random.randn(100).cumsum()\n",
    "    \n",
    "    # Plot results\n",
    "    plot_dataset_distribution(X, y, name, axs[i, 0])\n",
    "    plot_cost_evolution(costs, name, axs[i, 1])\n",
    "    plot_feature_comparison(original_features, optimized_features, name, axs[i, 2])\n",
    "    plot_variance_and_orthogonality(X_transformed, name, axs[i, 3])\n",
    "    \n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40aef48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as pnp\n",
    "from scipy.linalg import eigh\n",
    "from sklearn.datasets import load_iris, load_digits, load_wine, load_breast_cancer, fetch_olivetti_faces\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the quantum device\n",
    "dev = qml.device('default.qubit', wires=4)\n",
    "\n",
    "# Quantum circuit for slow feature function\n",
    "@qml.qnode(dev)\n",
    "def quantum_circuit(params, wires):\n",
    "    for l in range(len(params) // 2):\n",
    "        qml.RY(params[2 * l], wires=wires[l])\n",
    "        qml.RZ(params[2 * l + 1], wires=wires[l])\n",
    "        if l > 0:\n",
    "            qml.CNOT(wires=[l - 1, l])\n",
    "    return qml.expval(qml.PauliZ(0))\n",
    "\n",
    "# Cost function to be optimized using VQAs\n",
    "def cost_function(params, X):\n",
    "    num_samples = X.shape[0]\n",
    "    cost = 0\n",
    "    for i in range(num_samples):\n",
    "        X_i = X[i]\n",
    "        exp_value = quantum_circuit(params, wires=[0, 1, 2, 3])\n",
    "        cost += (np.mean(X_i) - exp_value) ** 2\n",
    "    return cost\n",
    "\n",
    "# Function to optimize quantum circuit parameters\n",
    "def optimize_qc_params(X, initial_params, learning_rate=0.01, epochs=100):\n",
    "    params = pnp.array(initial_params, requires_grad=True)\n",
    "    opt = qml.GradientDescentOptimizer(stepsize=learning_rate)\n",
    "    costs = []\n",
    "    for epoch in range(epochs):\n",
    "        params, cost = opt.step_and_cost(lambda p: cost_function(p, X), params)\n",
    "        costs.append(cost)\n",
    "    return params, costs\n",
    "\n",
    "# Calculate covariance matrix Σ\n",
    "def calculate_covariance_matrix(data):\n",
    "    mean_data = np.mean(data, axis=0)\n",
    "    centered_data = data - mean_data\n",
    "    covariance_matrix = np.cov(centered_data, rowvar=False)\n",
    "    return covariance_matrix\n",
    "\n",
    "# Calculate Laplacian matrix L = D - Σ\n",
    "def calculate_laplacian(covariance_matrix):\n",
    "    D = np.diag(np.sum(covariance_matrix, axis=1))\n",
    "    L = D - covariance_matrix\n",
    "    return L\n",
    "\n",
    "# Calculate the eigenvalues and eigenvectors\n",
    "def eigen_decomposition(matrix):\n",
    "    eigenvalues, eigenvectors = eigh(matrix)\n",
    "    return eigenvalues, eigenvectors\n",
    "\n",
    "# Project data onto the subspace of slowest eigenvectors\n",
    "def project_data(X, eigenvectors, num_slowest):\n",
    "    return np.dot(X, eigenvectors[:, -num_slowest:])\n",
    "\n",
    "# Main function to perform qSFA\n",
    "def qSFA(X, num_slowest=2, learning_rate=0.01, epochs=100):\n",
    "    # Preprocess data\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    \n",
    "    # Calculate covariance matrix Σ\n",
    "    covariance_matrix = calculate_covariance_matrix(X)\n",
    "    \n",
    "    # Calculate Laplacian matrix L\n",
    "    L = calculate_laplacian(covariance_matrix)\n",
    "    \n",
    "    # Eigen decomposition of the Laplacian matrix\n",
    "    eigenvalues, eigenvectors = eigen_decomposition(L)\n",
    "    \n",
    "    # Project data onto the subspace spanned by the slowest eigenvectors\n",
    "    X_projected = project_data(X, eigenvectors, num_slowest)\n",
    "    \n",
    "    # Optimize quantum circuit parameters\n",
    "    initial_params = np.random.uniform(0, 2 * np.pi, size=8)\n",
    "    optimized_params, costs = optimize_qc_params(X_projected, initial_params, learning_rate, epochs)\n",
    "    \n",
    "    return X_projected, optimized_params, costs\n",
    "\n",
    "# Define datasets\n",
    "datasets = {\n",
    "    'iris': load_iris(),\n",
    "    'digits': load_digits(),\n",
    "    'wine': load_wine(),\n",
    "    'breast_cancer': load_breast_cancer(),\n",
    "    'olivetti_faces': fetch_olivetti_faces()\n",
    "}\n",
    "\n",
    "# Plotting function for results\n",
    "def plot_combined_results(datasets):\n",
    "    num_datasets = len(datasets)\n",
    "    fig, axes = plt.subplots(2, num_datasets, figsize=(15, 10))\n",
    "\n",
    "    for i, (name, dataset) in enumerate(datasets.items()):\n",
    "        X = dataset.data\n",
    "        num_samples, num_features = X.shape\n",
    "        num_slowest = min(2, num_features)  # Ensure there are enough features to extract slow ones\n",
    "        \n",
    "        X_projected, optimized_params, costs = qSFA(X, num_slowest=num_slowest)\n",
    "        \n",
    "        # Cost Function Evolution\n",
    "        ax1 = axes[0, i]\n",
    "        ax1.plot(costs)\n",
    "        ax1.set_title(f'{name} - Cost Function Evolution')\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Cost')\n",
    "        \n",
    "        # Projected Data\n",
    "        ax2 = axes[1, i]\n",
    "        ax2.scatter(X_projected[:, 0], X_projected[:, 1])\n",
    "        ax2.set_title(f'{name} - Projected Data')\n",
    "        ax2.set_xlabel('Slow Feature 1')\n",
    "        ax2.set_ylabel('Slow Feature 2')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Generate and save combined plots\n",
    "plot_combined_results(datasets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8618d56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_iris, load_digits, load_wine, load_breast_cancer, fetch_olivetti_faces\n",
    "\n",
    "def load_and_preprocess_data(dataset_name, n_components=50):\n",
    "    # Load the dataset\n",
    "    data_loaders = {\n",
    "        'iris': load_iris,\n",
    "        'digits': load_digits,\n",
    "        'wine': load_wine,\n",
    "        'breast_cancer': load_breast_cancer,\n",
    "        'olivetti_faces': fetch_olivetti_faces\n",
    "    }\n",
    "    \n",
    "    data = data_loaders[dataset_name]()\n",
    "    X, y = data.data, data.target\n",
    "    \n",
    "    # Apply PCA for dimensionality reduction\n",
    "    pca = PCA(n_components=min(n_components, X.shape[1]))\n",
    "    X_reduced = pca.fit_transform(X)\n",
    "    \n",
    "    return X_reduced, y\n",
    "\n",
    "def encode_classical_to_quantum(X):\n",
    "    norms = np.linalg.norm(X, axis=1, keepdims=True)\n",
    "    return X / norms\n",
    "\n",
    "def qGM(U, V, psi, alpha, beta):\n",
    "    psi = np.reshape(psi, (psi.shape[-2], psi.shape[-1]))\n",
    "    U_psi = np.dot(U, psi)\n",
    "    V_psi = np.dot(V, psi)\n",
    "    return alpha @ U_psi + beta @ V_psi\n",
    "\n",
    "def feature_preservation_metric(U, V, alpha, beta, psi, theta, G_theta):\n",
    "    original = np.array([qGM(U, V, p, alpha, beta) for p in psi])\n",
    "    generated = np.array([G_theta(o) for o in original])\n",
    "    \n",
    "    return np.sum(np.trace(np.dot(original[i].conj().T, generated[i])) for i in range(len(original)))\n",
    "\n",
    "def optimize_qGM(X, U, V, alpha, beta, gamma, delta, n_iterations=10, learning_rate=0.01):\n",
    "    theta = np.random.rand(U[0].shape[0], U[0].shape[1])\n",
    "\n",
    "    for _ in range(n_iterations):\n",
    "        psi_x = np.array([np.outer(x, x) for x in X])\n",
    "        G_theta_psi_x = np.array([qGM(U, V, psi, alpha, beta) for psi in psi_x])\n",
    "        F_preservation = feature_preservation_metric(U, V, alpha, beta, psi_x, theta, lambda x: qGM(U, V, x, alpha, beta))\n",
    "        F_reconstruction = 0\n",
    "        \n",
    "        loss = gamma * F_preservation + delta * F_reconstruction\n",
    "\n",
    "        U -= learning_rate * np.random.randn(*U.shape)\n",
    "        V -= learning_rate * np.random.randn(*V.shape)\n",
    "        theta -= learning_rate * np.random.randn(*theta.shape)\n",
    "\n",
    "    return U, V\n",
    "\n",
    "def run_analysis_on_datasets(datasets):\n",
    "    fig, axs = plt.subplots(1, len(datasets), figsize=(20, 5))\n",
    "\n",
    "    for i, dataset in enumerate(datasets):\n",
    "        X, y = load_and_preprocess_data(dataset, n_components=50)\n",
    "        X = encode_classical_to_quantum(X)\n",
    "\n",
    "        n, d = X.shape\n",
    "        U = np.array([np.eye(d) for _ in range(d)])\n",
    "        V = np.array([np.eye(d) for _ in range(d)])\n",
    "        alpha = np.random.rand(d)\n",
    "        beta = np.random.rand(d)\n",
    "\n",
    "        gamma = 0.5\n",
    "        delta = 0.5\n",
    "\n",
    "        U, V = optimize_qGM(X, U, V, alpha, beta, gamma, delta, n_iterations=10)\n",
    "\n",
    "        axs[i].scatter(X[:, 0], X[:, 1], c=y, cmap='viridis')\n",
    "        axs[i].set_title(f'{dataset.capitalize()} Dataset')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "datasets = ['iris', 'digits', 'wine', 'breast_cancer', 'olivetti_faces']\n",
    "run_analysis_on_datasets(datasets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae733b8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
