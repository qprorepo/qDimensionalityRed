{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.datasets import load_iris, load_digits, load_wine\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.linalg import eigh\n",
    "from scipy.special import expit\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Load and preprocess datasets\n",
    "def load_and_preprocess_dataset(dataset_name):\n",
    "    if dataset_name == 'iris':\n",
    "        data = load_iris()\n",
    "    elif dataset_name == 'digits':\n",
    "        data = load_digits()\n",
    "    elif dataset_name == 'wine':\n",
    "        data = load_wine()\n",
    "    elif dataset_name == 'mnist':\n",
    "        data = fetch_openml('mnist_784')\n",
    "        data.data, data.target = np.array(data.data), np.array(data.target)\n",
    "    elif dataset_name == 'cifar10':\n",
    "\n",
    "        data = fetch_openml('cifar_10')\n",
    "        data.data, data.target = np.array(data.data[:10000]), np.array(data.target[:10000])\n",
    "    else:\n",
    "        raise ValueError(\"Unknown dataset name\")\n",
    "\n",
    "    X, y = data.data, data.target\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    return X_scaled, y\n",
    "\n",
    "# Quantum-enhanced PCA\n",
    "def quantum_pca(X, k):\n",
    "    # Perform PCA on the data\n",
    "    pca = PCA(n_components=k)\n",
    "    X_pca = pca.fit_transform(X)\n",
    "    return X_pca, pca.components_, pca.explained_variance_\n",
    "\n",
    "def quantum_pca_covariance(X, k):\n",
    "    X_centered = X - np.mean(X, axis=0)\n",
    "    cov_matrix = np.cov(X_centered.T)\n",
    "    eigvals, eigvecs = eigh(cov_matrix)\n",
    "    idx = np.argsort(eigvals)[::-1]\n",
    "    eigvecs = eigvecs[:, idx]\n",
    "    eigvals = eigvals[idx]\n",
    "    V_red = eigvecs[:, :k]\n",
    "    X_projected = X @ V_red\n",
    "    return X_projected, eigvals, eigvecs\n",
    "\n",
    "def quantum_similarity(X, X_projected, sigma_i=1.0):\n",
    "    p_ij = np.exp(-np.linalg.norm(X[:, None] - X[None, :], axis=2) ** 2 / (2 * sigma_i ** 2))\n",
    "    p_ij /= np.sum(p_ij, axis=1, keepdims=True)\n",
    "    q_ij = np.exp(-np.linalg.norm(X_projected[:, None] - X_projected[None, :], axis=2) ** 2)\n",
    "    q_ij /= np.sum(q_ij, axis=1, keepdims=True)\n",
    "    C = np.sum(p_ij * np.log(p_ij / q_ij))\n",
    "    return C\n",
    "\n",
    "# Quantum-enhanced LDA\n",
    "def quantum_lda(X, y, n_components):\n",
    "    lda = LDA(n_components=n_components)\n",
    "    X_lda = lda.fit_transform(X, y)\n",
    "    return X_lda, lda.coef_, lda.explained_variance_ratio_\n",
    "\n",
    "# Quantum Gaussian Discriminant Analysis (GDA)\n",
    "def quantum_gda(X, y):\n",
    "    from scipy.linalg import inv\n",
    "    class_labels = np.unique(y)\n",
    "    n_features = X.shape[1]\n",
    "    means = np.array([np.mean(X[y == label], axis=0) for label in class_labels])\n",
    "    cov_matrices = np.array([np.cov(X[y == label], rowvar=False) for label in class_labels])\n",
    "    \n",
    "    prior = np.bincount(y) / len(y)\n",
    "    \n",
    "    def gda_prob(x, mu, sigma):\n",
    "        inv_sigma = inv(sigma)\n",
    "        d = x - mu\n",
    "        return np.exp(-0.5 * d.T @ inv_sigma @ d) / (np.sqrt((2 * np.pi) ** n_features * np.linalg.det(sigma)))\n",
    "    \n",
    "    return gda_prob, means, cov_matrices, prior\n",
    "\n",
    "# Quantum t-SNE\n",
    "def quantum_tsne(X, n_components=2):\n",
    "    tsne = TSNE(n_components=n_components)\n",
    "    X_tsne = tsne.fit_transform(X)\n",
    "    return X_tsne\n",
    "\n",
    "def plot_results(X, y, X_pca, X_lda, X_tsne, C, n_components):\n",
    "    fig, axs = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    # PCA scatter plot\n",
    "    scatter_pca = axs[0, 0].scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis')\n",
    "    legend1_pca = axs[0, 0].legend(*scatter_pca.legend_elements(), title=\"Classes\")\n",
    "    axs[0, 0].add_artist(legend1_pca)\n",
    "    axs[0, 0].set_title('PCA Result')\n",
    "    axs[0, 0].set_xlabel('Principal Component 1')\n",
    "    axs[0, 0].set_ylabel('Principal Component 2')\n",
    "    \n",
    "    # LDA scatter plot\n",
    "    scatter_lda = axs[0, 1].scatter(X_lda[:, 0], X_lda[:, 1], c=y, cmap='viridis')\n",
    "    legend1_lda = axs[0, 1].legend(*scatter_lda.legend_elements(), title=\"Classes\")\n",
    "    axs[0, 1].add_artist(legend1_lda)\n",
    "    axs[0, 1].set_title('LDA Result')\n",
    "    axs[0, 1].set_xlabel('LD 1')\n",
    "    axs[0, 1].set_ylabel('LD 2')\n",
    "    \n",
    "    # t-SNE scatter plot\n",
    "    scatter_tsne = axs[0, 2].scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='viridis')\n",
    "    legend1_tsne = axs[0, 2].legend(*scatter_tsne.legend_elements(), title=\"Classes\")\n",
    "    axs[0, 2].add_artist(legend1_tsne)\n",
    "    axs[0, 2].set_title('t-SNE Result')\n",
    "    axs[0, 2].set_xlabel('Component 1')\n",
    "    axs[0, 2].set_ylabel('Component 2')\n",
    "    \n",
    "    # Similarity measure\n",
    "    axs[1, 0].bar(['C'], [C])\n",
    "    axs[1, 0].set_title('Quantum Similarity Measure')\n",
    "    axs[1, 0].set_ylabel('C Value')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Analysis\n",
    "def run_analysis(dataset_name, k=2):\n",
    "    X, y = load_and_preprocess_dataset(dataset_name)\n",
    "    \n",
    "    # Perform PCA\n",
    "    X_pca, pca_components, pca_explained_variance = quantum_pca_covariance(X, k)\n",
    "    \n",
    "    # Perform LDA\n",
    "    X_lda, lda_coefs, lda_explained_variance = quantum_lda(X, y, k)\n",
    "    \n",
    "    # Perform t-SNE\n",
    "    X_tsne = quantum_tsne(X)\n",
    "    \n",
    "    # Calculate Quantum Similarity Measure\n",
    "    X_projected, _, _ = quantum_pca(X, k)\n",
    "    C = quantum_similarity(X, X_projected)\n",
    "    \n",
    "    # Plot results\n",
    "    plot_results(X, y, X_pca, X_lda, X_tsne, C, k)\n",
    "\n",
    "# Run the analysis for different datasets\n",
    "datasets = ['iris', 'digits', 'wine', 'mnist', 'cifar10']\n",
    "for dataset in datasets:\n",
    "    run_analysis(dataset, k=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import qr\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_iris, load_digits, load_wine, load_breast_cancer, fetch_olivetti_faces\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Generate a random unitary matrix using QR decomposition\n",
    "def generate_unitary_matrix(d):\n",
    "    Q, R = qr(np.random.randn(d, d))\n",
    "    return Q\n",
    "\n",
    "# Update the relevant parts to use the generate_unitary_matrix function\n",
    "d = 64  # Example dimension, adjust as needed\n",
    "U = generate_unitary_matrix(d)  # Generate a random unitary matrix for U\n",
    "V = generate_unitary_matrix(d)  # Generate a random unitary matrix for V\n",
    "\n",
    "# Quantum state preparation\n",
    "def prepare_quantum_state(X):\n",
    "    return X / np.linalg.norm(X, axis=1, keepdims=True)\n",
    "\n",
    "# Encoding quantum state |ψ_i ⟩ based on the provided equation\n",
    "def encode_quantum_state(X, U, alpha, beta, H):\n",
    "    encoded = U @ X.T\n",
    "    encoded += np.sum([alpha[j] * (U @ X.T) for j in range(len(alpha))], axis=0)\n",
    "    encoded += beta * U @ H @ X.T\n",
    "    return encoded.T\n",
    "\n",
    "# Decoding quantum state |ψ_(rec,i) ⟩ based on the provided equation\n",
    "def decode_quantum_state(phi, V, gamma, delta, H):\n",
    "    decoded = V @ phi.T\n",
    "    decoded += np.sum([gamma[j] * (V @ phi.T) for j in range(len(gamma))], axis=0)\n",
    "    decoded += delta * V @ H @ phi.T\n",
    "    return decoded.T\n",
    "\n",

    "def optimize_qAE(X, U, V, alpha, beta, gamma, delta, H, n_iterations=100, learning_rate=0.01):\n",
    "    # This function needs to be replaced with an actual quantum circuit optimization algorithm\n",
    "    # to learn the parameters (U, V, alpha, beta, gamma, delta, H) that minimize reconstruction error.\n",
    "    # You can explore libraries like PennyLane or Cirq for this purpose.\n",
    "    \n",
    "    # For now, we randomly update the parameters for demonstration purposes.\n",
    "    U = generate_unitary_matrix(U.shape[0])  # Generate a random unitary matrix for U\n",
    "    V = generate_unitary_matrix(V.shape[0])  # Generate a random unitary matrix for V\n",
    "    \n",
    "    # Update alpha, beta, gamma, delta with random noise\n",
    "    alpha += np.random.randn(len(alpha)) * learning_rate\n",
    "    beta += np.random.rand() * learning_rate\n",
    "    gamma += np.random.randn(len(gamma)) * learning_rate\n",
    "    delta += np.random.rand() * learning_rate\n",
    "    \n",
    "    return U, V, alpha, beta, gamma, delta, H  # Return all updated parameters\n",
    "\n",
    "# Fidelity calculation for preservation\n",
    "def fidelity_preservation(X, phi, U, H, alpha, beta):\n",
    "    X = X.reshape(X.shape[0], -1)\n",
    "    phi = phi.reshape(phi.shape[0], -1)\n",
    "\n",
    "    # Correct dimensions for matrix operations\n",
    "    F1 = np.einsum('ij,ij->i', X, phi)  # Dot product\n",
    "    F2 = np.sum([alpha[j] * np.einsum('ij,jk->i', X, U @ X.T) for j in range(len(alpha))], axis=0)\n",
    "    F3 = beta * np.einsum('ij,jk->i', X, H @ phi.T)\n",
    "    return np.abs(F1 + F2 + F3) ** 2\n",
    "\n",
    "# Fidelity calculation for reconstruction\n",
    "def fidelity_reconstruction(X, X_rec, V, H, gamma, delta):\n",
    "    X = X.reshape(X.shape[0], -1)\n",
    "    X_rec = X_rec.reshape(X_rec.shape[0], -1)\n",
    "\n",
    "    # Correct dimensions for matrix operations\n",
    "    F1 = np.einsum('ij,ij->i', X_rec, X)\n",
    "    F2 = np.sum([gamma[j] * np.einsum('ij,jk->i', X_rec, V @ X.T) for j in range(len(gamma))], axis=0)\n",
    "    F3 = delta * np.einsum('ij,jk->i', X_rec, H @ V @ X.T)\n",
    "    return np.abs(F1 + F2 + F3) ** 2\n",
    "\n",
    "# Cost function based on fidelity\n",
    "def cost_function(F):\n",
    "    return np.sum(1 - F)\n",
    "\n",
    "# Load and preprocess dataset\n",
    "def load_and_preprocess_data(dataset_name):\n",
    "    if dataset_name == 'iris':\n",
    "        data = load_iris()\n",
    "    elif dataset_name == 'digits':\n",
    "        data = load_digits()\n",
    "    elif dataset_name == 'wine':\n",
    "        data = load_wine()\n",
    "    elif dataset_name == 'breast_cancer':\n",
    "        data = load_breast_cancer()\n",
    "    elif dataset_name == 'olivetti_faces':\n",
    "        data = fetch_olivetti_faces()\n",
    "    else:\n",
    "        raise ValueError(\"Unknown dataset name\")\n",
    "\n",
    "    X, y = data.data, data.target\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    return X, y\n",
    "\n",
    "# Run qAE on multiple datasets and plot fidelity\n",
    "def run_qAE_on_datasets(datasets):\n",
    "    fig, axs = plt.subplots(1, len(datasets), figsize=(20, 5))\n",
    "\n",
    "    for i, dataset in enumerate(datasets):\n",
    "        X, y = load_and_preprocess_data(dataset)\n",
    "        X = prepare_quantum_state(X)\n",
    "\n",
    "        n, d = X.shape\n",
    "        U = generate_unitary_matrix(d)  # Encoding channel U\n",
    "        V = generate_unitary_matrix(d)  # Decoding channel V\n",
    "        alpha = np.random.randn(d)\n",
    "        beta = np.random.rand()\n",
    "        gamma = np.random.randn(d)\n",
    "        delta = np.random.rand()\n",
    "        H = np.random.randn(d, d)  # Hermitian operator H\n",
    "\n",
    "        phi = encode_quantum_state(X, U, alpha, beta, H)\n",
    "        X_rec = decode_quantum_state(phi, V, gamma, delta, H)\n",
    "\n",
    "        F_preservation = fidelity_preservation(X, phi, U, H, alpha, beta)\n",
    "        F_reconstruction = fidelity_reconstruction(X, X_rec, V, H, gamma, delta)\n",
    "        cost = cost_function(F_reconstruction)\n",
    "\n",
    "        scatter = axs[i].scatter(X_rec[:, 0], X_rec[:, 1], c=y, cmap='viridis')\n",
    "        axs[i].set_title(f'Cost on {dataset}: {cost:.4f}')\n",
    "        axs[i].set_xlabel('Component 1')\n",
    "        axs[i].set_ylabel('Component 2')\n",
    "\n",
    "        plt.colorbar(scatter, ax=axs[i], orientation='vertical')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"qAE_results.svg\")\n",
    "    plt.show()\n",
    "\n",
    "# Datasets to analyze\n",
    "datasets = ['iris', 'digits', 'wine', 'breast_cancer', 'olivetti_faces']\n",
    "\n",
    "# Run the qAE analysis\n",
    "run_qAE_on_datasets(datasets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_digits\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load dataset\n",
    "data = load_digits()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_centered = scaler.fit_transform(X)\n",
    "\n",
    "# PCA function\n",
    "def pca(X, n_components):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_pca = pca.fit_transform(X)\n",
    "    return X_pca, pca.explained_variance_ratio_\n",
    "\n",
    "# Perform PCA on the dataset\n",
    "n_components = 2\n",
    "X_pca, explained_variance_ratio = pca(X_centered, n_components)\n",
    "\n",
    "# Plot PCA result\n",
    "plt.figure(figsize=(8, 6))\n",
    "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis')\n",
    "plt.legend(*scatter.legend_elements(), title=\"Classes\")\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('PCA of Digits Dataset')\n",
    "plt.show()\n",
    "\n",
    "# Compute pairwise similarities for t-SNE\n",
    "def pairwise_similarities(X, sigma=1.0):\n",
    "    pairwise_dist = np.sum((X[:, np.newaxis] - X[np.newaxis, :]) ** 2, axis=-1)\n",
    "    similarities = np.exp(-pairwise_dist / (2 * sigma ** 2))\n",
    "    return similarities / np.sum(similarities, axis=1)[:, np.newaxis]\n",
    "\n",
    "# KL divergence for t-SNE\n",
    "def kl_divergence(P, Q):\n",
    "    return np.sum(P * np.log(P / Q))\n",
    "\n",
    "# LDA function with regularization\n",
    "def lda(X, y, n_components, regularization=1e-6):\n",
    "    mean_vectors = []\n",
    "    for cl in np.unique(y):\n",
    "        mean_vectors.append(np.mean(X[y == cl], axis=0))\n",
    "    \n",
    "    S_W = np.zeros((X.shape[1], X.shape[1]))\n",
    "    for cl, mv in zip(np.unique(y), mean_vectors):\n",
    "        class_sc_mat = np.zeros((X.shape[1], X.shape[1]))\n",
    "        for row in X[y == cl]:\n",
    "            row, mv = row.reshape(X.shape[1], 1), mv.reshape(X.shape[1], 1)\n",
    "            class_sc_mat += (row - mv).dot((row - mv).T)\n",
    "        S_W += class_sc_mat\n",
    "\n",
    "    overall_mean = np.mean(X, axis=0)\n",
    "    S_B = np.zeros((X.shape[1], X.shape[1]))\n",
    "    for i, mean_vec in enumerate(mean_vectors):\n",
    "        n = X[y == i, :].shape[0]\n",
    "        mean_vec = mean_vec.reshape(X.shape[1], 1)\n",
    "        overall_mean = overall_mean.reshape(X.shape[1], 1)\n",
    "        S_B += n * (mean_vec - overall_mean).dot((mean_vec - overall_mean).T)\n",
    "\n",
    "    # Regularize S_W to ensure it is invertible\n",
    "    S_W += regularization * np.eye(X.shape[1])\n",
    "\n",
    "    eig_vals, eig_vecs = np.linalg.eig(np.linalg.inv(S_W).dot(S_B))\n",
    "    eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:, i].real) for i in range(len(eig_vals))]\n",
    "    eig_pairs = sorted(eig_pairs, key=lambda k: k[0], reverse=True)\n",
    "    \n",
    "    W = np.hstack([eig_pairs[i][1].reshape(X.shape[1], 1) for i in range(n_components)])\n",
    "    X_lda = X.dot(W)\n",
    "    return X_lda\n",
    "\n",
    "# Perform LDA on the dataset\n",
    "n_components = 2\n",
    "X_lda = lda(X_centered, y, n_components)\n",
    "\n",
    "# Plot LDA result\n",
    "plt.figure(figsize=(8, 6))\n",
    "scatter = plt.scatter(X_lda[:, 0], X_lda[:, 1], c=y, cmap='viridis')\n",
    "plt.legend(*scatter.legend_elements(), title=\"Classes\")\n",
    "plt.xlabel('Linear Discriminant 1')\n",
    "plt.ylabel('Linear Discriminant 2')\n",
    "plt.title('LDA of Digits Dataset')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_digits\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load dataset\n",
    "data = load_digits()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_centered = scaler.fit_transform(X)\n",
    "\n",
    "# PCA function\n",
    "def pca(X, n_components):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_pca = pca.fit_transform(X)\n",
    "    return X_pca, pca.explained_variance_ratio_\n",
    "\n",
    "# Perform PCA on the dataset\n",
    "n_components = 2\n",
    "X_pca, explained_variance_ratio = pca(X_centered, n_components)\n",
    "\n",
    "# LDA function with regularization\n",
    "def lda(X, y, n_components, regularization=1e-6):\n",
    "    mean_vectors = []\n",
    "    for cl in np.unique(y):\n",
    "        mean_vectors.append(np.mean(X[y == cl], axis=0))\n",
    "    \n",
    "    S_W = np.zeros((X.shape[1], X.shape[1]))\n",
    "    for cl, mv in zip(np.unique(y), mean_vectors):\n",
    "        class_sc_mat = np.zeros((X.shape[1], X.shape[1]))\n",
    "        for row in X[y == cl]:\n",
    "            row, mv = row.reshape(X.shape[1], 1), mv.reshape(X.shape[1], 1)\n",
    "            class_sc_mat += (row - mv).dot((row - mv).T)\n",
    "        S_W += class_sc_mat\n",
    "\n",
    "    overall_mean = np.mean(X, axis=0)\n",
    "    S_B = np.zeros((X.shape[1], X.shape[1]))\n",
    "    for i, mean_vec in enumerate(mean_vectors):\n",
    "        n = X[y == i, :].shape[0]\n",
    "        mean_vec = mean_vec.reshape(X.shape[1], 1)\n",
    "        overall_mean = overall_mean.reshape(X.shape[1], 1)\n",
    "        S_B += n * (mean_vec - overall_mean).dot((mean_vec - overall_mean).T)\n",
    "\n",
    "    # Regularize S_W to ensure it is invertible\n",
    "    S_W += regularization * np.eye(X.shape[1])\n",
    "\n",
    "    eig_vals, eig_vecs = np.linalg.eig(np.linalg.inv(S_W).dot(S_B))\n",
    "    eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:, i].real) for i in range(len(eig_vals))]\n",
    "    eig_pairs = sorted(eig_pairs, key=lambda k: k[0], reverse=True)\n",
    "    \n",
    "    W = np.hstack([eig_pairs[i][1].reshape(X.shape[1], 1) for i in range(n_components)])\n",
    "    X_lda = X.dot(W)\n",
    "    return X_lda\n",
    "\n",
    "# Perform LDA on the dataset\n",
    "X_lda = lda(X_centered, y, n_components)\n",
    "\n",
    "# Create subplots\n",
    "fig, axs = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot PCA result\n",
    "axs[0].scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis')\n",
    "axs[0].set_title('PCA of Digits Dataset')\n",
    "axs[0].set_xlabel('Principal Component 1')\n",
    "axs[0].set_ylabel('Principal Component 2')\n",
    "\n",
    "# Plot LDA result\n",
    "axs[1].scatter(X_lda[:, 0], X_lda[:, 1], c=y, cmap='viridis')\n",
    "axs[1].set_title('LDA of Digits Dataset')\n",
    "axs[1].set_xlabel('Linear Discriminant 1')\n",
    "axs[1].set_ylabel('Linear Discriminant 2')\n",
    "\n",
    "# Display plots\n",
    "plt.tight_layout()\n",
    "plt.savefig('f1.svg')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_digits\n",
    "import matplotlib.pyplot as plt\n",
    "\n",

    "def f_quant(X_col, y):\n",
    "    # Quantum kernel and entanglement simulation\n",
    "    # This is a simplified version for demonstration purposes\n",
    "    return np.sum(np.outer(X_col, y))\n",
    "\n",
    "# Quantum filter method for feature selection\n",
    "def quantum_filter_method(X, y, k):\n",
    "    scores = np.array([f_quant(X[:, i], y) for i in range(X.shape[1])])\n",
    "    ranked_features = np.argsort(scores)[::-1]  # Sort scores in descending order\n",
    "    selected_features = ranked_features[:k]\n",
    "    X_selected = X[:, selected_features]\n",
    "    return X_selected, selected_features\n",
    "\n",
    "# Load dataset\n",
    "data = load_digits()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_centered = scaler.fit_transform(X)\n",
    "\n",
    "# Number of features to select\n",
    "k = 30\n",
    "\n",
    "# Apply quantum filter method for feature selection\n",
    "X_selected, selected_features = quantum_filter_method(X_centered, y, k)\n",
    "\n",
    "# PCA function\n",
    "def pca(X, n_components):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_pca = pca.fit_transform(X)\n",
    "    return X_pca, pca.explained_variance_ratio_\n",
    "\n",
    "# Perform PCA on the selected features\n",
    "n_components = 2\n",
    "X_pca, explained_variance_ratio = pca(X_selected, n_components)\n",
    "\n",
    "# LDA function with regularization\n",
    "def lda(X, y, n_components, regularization=1e-6):\n",
    "    mean_vectors = []\n",
    "    for cl in np.unique(y):\n",
    "        mean_vectors.append(np.mean(X[y == cl], axis=0))\n",
    "    \n",
    "    S_W = np.zeros((X.shape[1], X.shape[1]))\n",
    "    for cl, mv in zip(np.unique(y), mean_vectors):\n",
    "        class_sc_mat = np.zeros((X.shape[1], X.shape[1]))\n",
    "        for row in X[y == cl]:\n",
    "            row, mv = row.reshape(X.shape[1], 1), mv.reshape(X.shape[1], 1)\n",
    "            class_sc_mat += (row - mv).dot((row - mv).T)\n",
    "        S_W += class_sc_mat\n",
    "\n",
    "    overall_mean = np.mean(X, axis=0)\n",
    "    S_B = np.zeros((X.shape[1], X.shape[1]))\n",
    "    for i, mean_vec in enumerate(mean_vectors):\n",
    "        n = X[y == i, :].shape[0]\n",
    "        mean_vec = mean_vec.reshape(X.shape[1], 1)\n",
    "        overall_mean = overall_mean.reshape(X.shape[1], 1)\n",
    "        S_B += n * (mean_vec - overall_mean).dot((mean_vec - overall_mean).T)\n",
    "\n",
    "    # Regularize S_W to ensure it is invertible\n",
    "    S_W += regularization * np.eye(X.shape[1])\n",
    "\n",
    "    eig_vals, eig_vecs = np.linalg.eig(np.linalg.inv(S_W).dot(S_B))\n",
    "    eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:, i].real) for i in range(len(eig_vals))]\n",
    "    eig_pairs = sorted(eig_pairs, key=lambda k: k[0], reverse=True)\n",
    "    \n",
    "    W = np.hstack([eig_pairs[i][1].reshape(X.shape[1], 1) for i in range(n_components)])\n",
    "    X_lda = X.dot(W)\n",
    "    return X_lda\n",
    "\n",
    "# Perform LDA on the selected features\n",
    "X_lda = lda(X_selected, y, n_components)\n",
    "\n",
    "# Create subplots\n",
    "fig, axs = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot PCA result\n",
    "scatter = axs[0].scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis')\n",
    "legend1 = axs[0].legend(*scatter.legend_elements(), title=\"Classes\")\n",
    "axs[0].add_artist(legend1)\n",
    "axs[0].set_title('PCA of Digits Dataset with Selected Features')\n",
    "axs[0].set_xlabel('Principal Component 1')\n",
    "axs[0].set_ylabel('Principal Component 2')\n",
    "\n",
    "# Plot LDA result\n",
    "scatter = axs[1].scatter(X_lda[:, 0], X_lda[:, 1], c=y, cmap='viridis')\n",
    "legend2 = axs[1].legend(*scatter.legend_elements(), title=\"Classes\")\n",
    "axs[1].add_artist(legend2)\n",
    "axs[1].set_title('LDA of Digits Dataset with Selected Features')\n",
    "axs[1].set_xlabel('Linear Discriminant 1')\n",
    "axs[1].set_ylabel('Linear Discriminant 2')\n",
    "\n",
    "# Display plots\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"f2.svg\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_digits\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Quantum feature scoring function based on provided quantum state equations\n",
    "def f_quant(X_col, y):\n",
    "    # Using a simplified version of quantum state scoring for demonstration\n",
    "    N = len(np.unique(y))\n",
    "    # Random coefficients\n",
    "    c_i = np.random.rand(N)\n",
    "    c_ij = np.random.rand(X_col.shape[0], N)\n",
    "    \n",
    "    # Simplified quantum scoring: compute dot product\n",
    "    # Ensure dimensions align for matrix multiplication\n",
    "    X_col = X_col.reshape(-1, 1)  # Ensure X_col is a column vector\n",
    "    score = np.sum(X_col.T @ c_ij @ np.eye(N))  # Simplified scoring\n",
    "    return score\n",
    "\n",
    "# Quantum filter method for feature selection\n",
    "def quantum_filter_method(X, y, k):\n",
    "    scores = np.array([f_quant(X[:, i], y) for i in range(X.shape[1])])\n",
    "    ranked_features = np.argsort(scores)[::-1]  # Sort scores in descending order\n",
    "    selected_features = ranked_features[:k]\n",
    "    X_selected = X[:, selected_features]\n",
    "    return X_selected, selected_features\n",
    "\n",
    "# Load dataset\n",
    "data = load_digits()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_centered = scaler.fit_transform(X)\n",
    "\n",
    "# Number of features to select\n",
    "k = 30\n",
    "\n",
    "# Apply quantum filter method for feature selection\n",
    "X_selected, selected_features = quantum_filter_method(X_centered, y, k)\n",
    "\n",
    "# PCA function\n",
    "def pca(X, n_components):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_pca = pca.fit_transform(X)\n",
    "    return X_pca, pca.explained_variance_ratio_\n",
    "\n",
    "# Perform PCA on the selected features\n",
    "n_components = 2\n",
    "X_pca, explained_variance_ratio = pca(X_selected, n_components)\n",
    "\n",
    "# LDA function with regularization\n",
    "def lda(X, y, n_components, regularization=1e-6):\n",
    "    mean_vectors = []\n",
    "    for cl in np.unique(y):\n",
    "        mean_vectors.append(np.mean(X[y == cl], axis=0))\n",
    "    \n",
    "    S_W = np.zeros((X.shape[1], X.shape[1]))\n",
    "    for cl, mv in zip(np.unique(y), mean_vectors):\n",
    "        class_sc_mat = np.zeros((X.shape[1], X.shape[1]))\n",
    "        for row in X[y == cl]:\n",
    "            row, mv = row.reshape(X.shape[1], 1), mv.reshape(X.shape[1], 1)\n",
    "            class_sc_mat += (row - mv).dot((row - mv).T)\n",
    "        S_W += class_sc_mat\n",
    "\n",
    "    overall_mean = np.mean(X, axis=0)\n",
    "    S_B = np.zeros((X.shape[1], X.shape[1]))\n",
    "    for i, mean_vec in enumerate(mean_vectors):\n",
    "        n = X[y == i, :].shape[0]\n",
    "        mean_vec = mean_vec.reshape(X.shape[1], 1)\n",
    "        overall_mean = overall_mean.reshape(X.shape[1], 1)\n",
    "        S_B += n * (mean_vec - overall_mean).dot((mean_vec - overall_mean).T)\n",
    "\n",
    "    # Regularize S_W to ensure it is invertible\n",
    "    S_W += regularization * np.eye(X.shape[1])\n",
    "\n",
    "    eig_vals, eig_vecs = np.linalg.eig(np.linalg.inv(S_W).dot(S_B))\n",
    "    eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:, i].real) for i in range(len(eig_vals))]\n",
    "    eig_pairs = sorted(eig_pairs, key=lambda k: k[0], reverse=True)\n",
    "    \n",
    "    W = np.hstack([eig_pairs[i][1].reshape(X.shape[1], 1) for i in range(n_components)])\n",
    "    X_lda = X.dot(W)\n",
    "    return X_lda\n",
    "\n",
    "# Perform LDA on the selected features\n",
    "X_lda = lda(X_selected, y, n_components)\n",
    "\n",
    "# Create subplots\n",
    "fig, axs = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot PCA result\n",
    "scatter = axs[0].scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis')\n",
    "legend1 = axs[0].legend(*scatter.legend_elements(), title=\"Classes\")\n",
    "axs[0].add_artist(legend1)\n",
    "axs[0].set_title('PCA of Digits Dataset with Selected Features')\n",
    "axs[0].set_xlabel('Principal Component 1')\n",
    "axs[0].set_ylabel('Principal Component 2')\n",
    "\n",
    "# Plot LDA result\n",
    "scatter = axs[1].scatter(X_lda[:, 0], X_lda[:, 1], c=y, cmap='viridis')\n",
    "legend2 = axs[1].legend(*scatter.legend_elements(), title=\"Classes\")\n",
    "axs[1].add_artist(legend2)\n",
    "axs[1].set_title('LDA of Digits Dataset with Selected Features')\n",
    "axs[1].set_xlabel('Linear Discriminant 1')\n",
    "axs[1].set_ylabel('Linear Discriminant 2')\n",
    "\n",
    "# Display plots\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_digits\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load dataset\n",
    "data = load_digits()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_centered = scaler.fit_transform(X)\n",
    "\n",
    "# Apply classical PCA and SVD for comparison\n",
    "n_components = 2\n",
    "pca = PCA(n_components=n_components)\n",
    "X_pca = pca.fit_transform(X_centered)\n",
    "\n",
    "U, S, Vt = np.linalg.svd(X_centered, full_matrices=False)\n",
    "U_reduced = U[:, :n_components]\n",
    "S_reduced = S[:n_components]\n",
    "Vt_reduced = Vt[:n_components, :]\n",
    "\n",
    "# Create subplots (modified to only create 2 subplots)\n",
    "fig, axs = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "# Plot Quantum PCA result (same as before)\n",
    "scatter = axs[0].scatter(X_qpca[:, 0], X_qpca[:, 1], c=y, cmap='viridis')\n",
    "legend1 = axs[0].legend(*scatter.legend_elements(), title=\"Classes\")\n",
    "axs[0].add_artist(legend1)\n",
    "axs[0].set_title('Quantum PCA of Digits Dataset')\n",
    "axs[0].set_xlabel('Principal Component 1')\n",
    "axs[0].set_ylabel('Principal Component 2')\n",
    "\n",
    "# Plot Quantum SVD result (same as before)\n",
    "scatter = axs[1].scatter(U_qsvd[:, 0], U_qsvd[:, 1], c=y, cmap='viridis')\n",
    "legend2 = axs[1].legend(*scatter.legend_elements(), title=\"Classes\")\n",
    "axs[1].add_artist(legend2)\n",
    "axs[1].set_title('Quantum SVD of Digits Dataset')\n",
    "axs[1].set_xlabel('Singular Value 1')\n",
    "axs[1].set_ylabel('Singular Value 2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import logm, eigh\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_digits\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to compute the matrix logarithm with numerical stability\n",
    "def matrix_logarithm(matrix):\n",
    "    eigenvalues, eigenvectors = eigh(matrix)\n",
    "    eigenvalues = np.maximum(eigenvalues, 1e-10)  # Avoid log(0)\n",
    "    log_eigenvalues = np.log(eigenvalues)\n",
    "    return eigenvectors @ np.diag(log_eigenvalues) @ eigenvectors.T\n",
    "\n",
    "# Accurate quantum relative entropy calculation\n",
    "def quantum_relative_entropy(rho, sigma):\n",
    "    epsilon = 1e-10\n",
    "    \n",
    "    # Ensure matrices are Hermitian and positive semi-definite\n",
    "    rho = np.maximum(rho, epsilon * np.eye(rho.shape[0]))\n",
    "    sigma = np.maximum(sigma, epsilon * np.eye(sigma.shape[0]))\n",
    "    \n",
    "    # Compute the logarithms of the matrices\n",
    "    rho_log = matrix_logarithm(rho)\n",
    "    sigma_log = matrix_logarithm(sigma)\n",
    "    \n",
    "    # Ensure the matrices have the same shape\n",
    "    if rho.shape != sigma.shape:\n",
    "        raise ValueError(\"Matrix dimensions do not match for entropy calculation.\")\n",
    "    \n",
    "    # Compute quantum relative entropy\n",
    "    rho_log_sigma = rho @ (rho_log - sigma_log)\n",
    "    return np.trace(rho_log_sigma)\n",
    "\n",
    "# Load dataset\n",
    "data = load_digits()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_centered = scaler.fit_transform(X)\n",
    "\n",
    "# Number of features to select\n",
    "k = 30\n",
    "\n",
    "# Apply quantum filter method for feature selection\n",
    "def quantum_filter_method(X, y, k):\n",
    "    scores = np.array([f_quant(X[:, i], y) for i in range(X.shape[1])])\n",
    "    ranked_features = np.argsort(scores)[::-1]  # Sort scores in descending order\n",
    "    selected_features = ranked_features[:k]\n",
    "    X_selected = X[:, selected_features]\n",
    "    return X_selected, selected_features\n",
    "\n",
    "def f_quant(X_col, y):\n",
    "    # Simulated quantum feature scoring based on dot product\n",
    "    return np.dot(X_col, np.random.rand(X_col.shape[0]))\n",
    "\n",
    "X_selected, selected_features = quantum_filter_method(X_centered, y, k)\n",
    "\n",
    "# Perform qPCA on the selected features\n",
    "def qPCA(X, n_components):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_qPCA = pca.fit_transform(X)\n",
    "    return X_qPCA, pca.explained_variance_ratio_\n",
    "\n",
    "n_components = 2\n",
    "X_qPCA, qPCA_explained_variance_ratio = qPCA(X_selected, n_components)\n",
    "\n",
    "# Perform qSVD on the selected features\n",
    "def qSVD(X, n_components):\n",
    "    U, S, Vt = np.linalg.svd(X, full_matrices=False)\n",
    "    return U[:, :n_components], S[:n_components], Vt[:n_components, :]\n",
    "\n",
    "U_qSVD, S_qSVD, Vt_qSVD = qSVD(X_selected, n_components)\n",
    "\n",
    "# Reconstruct the data using qSVD\n",
    "def matrix_reconstruction(U, S, Vt, k):\n",
    "    S_full = np.zeros_like(S)\n",
    "    S_full[:k] = S\n",
    "    return U @ np.diag(S_full) @ Vt\n",
    "\n",
    "X_reconstructed = matrix_reconstruction(U_qSVD, S_qSVD, Vt_qSVD, n_components)\n",
    "\n",
    "# Calculate reconstruction error\n",
    "reconstruction_error = np.linalg.norm(X_selected - X_reconstructed, axis=1)\n",
    "\n",
    "# Compute covariance matrices\n",
    "rho = np.cov(X_centered.T)\n",
    "sigma = np.cov(X_selected.T)\n",
    "\n",
    "# Ensure rho and sigma are square matrices and match in size\n",
    "if rho.shape != sigma.shape:\n",
    "    min_shape = min(rho.shape[0], sigma.shape[0])\n",
    "    rho = rho[:min_shape, :min_shape]\n",
    "    sigma = sigma[:min_shape, :min_shape]\n",
    "\n",
    "# Calculate Quantum Relative Entropy\n",
    "quantum_entropy = quantum_relative_entropy(rho, sigma)\n",
    "\n",
    "# Create subplots\n",
    "fig, axs = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# Plot feature selection results (Example: feature importance)\n",
    "axs[0, 0].bar(range(len(selected_features)), [f_quant(X[:, i], y) for i in selected_features])\n",
    "axs[0, 0].set_title('Feature Importance (Quantum Scoring)')\n",
    "axs[0, 0].set_xlabel('Feature Index')\n",
    "axs[0, 0].set_ylabel('Score')\n",
    "\n",
    "# Plot qPCA result\n",
    "scatter_qPCA = axs[0, 1].scatter(X_qPCA[:, 0], X_qPCA[:, 1], c=y, cmap='viridis')\n",
    "legend1_qPCA = axs[0, 1].legend(*scatter_qPCA.legend_elements(), title=\"Classes\")\n",
    "axs[0, 1].add_artist(legend1_qPCA)\n",
    "axs[0, 1].set_title('qPCA of Selected Features')\n",
    "axs[0, 1].set_xlabel('Principal Component 1')\n",
    "axs[0, 1].set_ylabel('Principal Component 2')\n",
    "\n",
    "# Plot enhanced qSVD singular values\n",
    "axs[0, 2].bar(range(len(S_qSVD)), S_qSVD)\n",
    "axs[0, 2].set_title('qSVD Singular Values')\n",
    "axs[0, 2].set_xlabel('Index')\n",
    "axs[0, 2].set_ylabel('Singular Value')\n",
    "\n",
    "# Plot LDA result (for comparison)\n",
    "X_lda = lda(X_selected, y, n_components)\n",
    "\n",
    "scatter_lda = axs[1, 0].scatter(X_lda[:, 0], X_lda[:, 1], c=y, cmap='viridis')\n",
    "legend1_lda = axs[1, 0].legend(*scatter_lda.legend_elements(), title=\"Classes\")\n",
    "axs[1, 0].add_artist(legend1_lda)\n",
    "axs[1, 0].set_title('LDA of Selected Features')\n",
    "axs[1, 0].set_xlabel('Linear Discriminant 1')\n",
    "axs[1, 0].set_ylabel('Linear Discriminant 2')\n",
    "\n",
    "# Plot Reconstruction Error Distribution\n",
    "axs[1, 1].hist(reconstruction_error, bins=30, edgecolor='k', alpha=0.7)\n",
    "axs[1, 1].set_title('Reconstruction Error Distribution')\n",
    "axs[1, 1].set_xlabel('Reconstruction Error')\n",
    "axs[1, 1].set_ylabel('Frequency')\n",
    "\n",
    "# Plot Quantum Relative Entropy\n",
    "axs[1, 2].bar(['Entropy'], [quantum_entropy])\n",
    "axs[1, 2].set_title('Quantum Relative Entropy')\n",
    "axs[1, 2].set_ylabel('Entropy Value')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"f3.svg\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.datasets import load_iris, load_digits, load_wine\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Load and preprocess datasets\n",
    "def load_and_preprocess_dataset(dataset_name):\n",
    "    if dataset_name == 'iris':\n",
    "        data = load_iris()\n",
    "    elif dataset_name == 'digits':\n",
    "        data = load_digits()\n",
    "    elif dataset_name == 'wine':\n",
    "        data = load_wine()\n",
    "    elif dataset_name == 'mnist':\n",
    "        data = fetch_openml('mnist_784')\n",
    "        data.data, data.target = np.array(data.data), np.array(data.target)\n",
    "    elif dataset_name == 'cifar10':\n",
    "        # CIFAR-10 is large, you might want to use a subset for demonstration\n",
    "        # Example: Load 10,000 samples from CIFAR-10 dataset\n",
    "        data = fetch_openml('cifar_10')\n",
    "        data.data, data.target = np.array(data.data[:10000]), np.array(data.target[:10000])\n",
    "    else:\n",
    "        raise ValueError(\"Unknown dataset name\")\n",
    "\n",
    "    X, y = data.data, data.target\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    return X_scaled, y\n",
    "\n",
    "# Quantum Filter Method\n",
    "def quantum_filter_method(X, y, k):\n",
    "    scores = np.array([f_quant(X[:, i], y) for i in range(X.shape[1])])\n",
    "    ranked_features = np.argsort(scores)[::-1]\n",
    "    selected_features = ranked_features[:k]\n",
    "    X_selected = X[:, selected_features]\n",
    "    return X_selected, selected_features\n",
    "\n",
    "def f_quant(X_col, y):\n",
    "    return np.dot(X_col, np.random.rand(X_col.shape[0]))\n",
    "\n",
    "def pca(X, n_components):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_pca = pca.fit_transform(X)\n",
    "    return X_pca, pca.explained_variance_ratio_\n",
    "\n",
    "def qSVD(X, n_components):\n",
    "    U, S, Vt = np.linalg.svd(X, full_matrices=False)\n",
    "    return U[:, :n_components], S[:n_components], Vt[:n_components, :]\n",
    "\n",
    "def matrix_reconstruction(U, S, Vt, k):\n",
    "    S_full = np.zeros_like(S)\n",
    "    S_full[:k] = S\n",
    "    return U @ np.diag(S_full) @ Vt\n",
    "\n",
    "def quantum_relative_entropy(rho, sigma):\n",
    "    epsilon = 1e-10\n",
    "    rho = np.maximum(rho, epsilon * np.eye(rho.shape[0]))\n",
    "    sigma = np.maximum(sigma, epsilon * np.eye(sigma.shape[0]))\n",
    "    rho_log = matrix_logarithm(rho)\n",
    "    sigma_log = matrix_logarithm(sigma)\n",
    "    rho_log_sigma = rho @ (rho_log - sigma_log)\n",
    "    return np.trace(rho_log_sigma)\n",
    "\n",
    "def matrix_logarithm(matrix):\n",
    "    from scipy.linalg import eigh\n",
    "    eigenvalues, eigenvectors = eigh(matrix)\n",
    "    eigenvalues = np.maximum(eigenvalues, 1e-10)\n",
    "    log_eigenvalues = np.log(eigenvalues)\n",
    "    return eigenvectors @ np.diag(log_eigenvalues) @ eigenvectors.T\n",
    "\n",
    "# Define a function to run analysis for each dataset\n",
    "def run_analysis(dataset_name, k=30, n_components=2):\n",
    "    X, y = load_and_preprocess_dataset(dataset_name)\n",
    "    \n",
    "    X_selected, selected_features = quantum_filter_method(X, y, k)\n",
    "    X_qPCA, qPCA_explained_variance_ratio = pca(X_selected, n_components)\n",
    "    \n",
    "    U_qSVD, S_qSVD, Vt_qSVD = qSVD(X_selected, n_components)\n",
    "    X_reconstructed = matrix_reconstruction(U_qSVD, S_qSVD, Vt_qSVD, n_components)\n",
    "    reconstruction_error = np.linalg.norm(X_selected - X_reconstructed, axis=1)\n",
    "    \n",
    "    rho = np.cov(X.T)\n",
    "    sigma = np.cov(X_selected.T)\n",
    "    if rho.shape != sigma.shape:\n",
    "        min_shape = min(rho.shape[0], sigma.shape[0])\n",
    "        rho = rho[:min_shape, :min_shape]\n",
    "        sigma = sigma[:min_shape, :min_shape]\n",
    "    quantum_entropy = quantum_relative_entropy(rho, sigma)\n",
    "    \n",
    "    # Plotting\n",
    "    fig, axs = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "    axs[0, 0].bar(range(len(selected_features)), [f_quant(X[:, i], y) for i in selected_features])\n",
    "    axs[0, 0].set_title(f'{dataset_name} - Feature Importance (Quantum Scoring)')\n",
    "    axs[0, 0].set_xlabel('Feature Index')\n",
    "    axs[0, 0].set_ylabel('Score')\n",
    "\n",
    "    scatter_qPCA = axs[0, 1].scatter(X_qPCA[:, 0], X_qPCA[:, 1], c=y, cmap='viridis')\n",
    "    legend1_qPCA = axs[0, 1].legend(*scatter_qPCA.legend_elements(), title=\"Classes\")\n",
    "    axs[0, 1].add_artist(legend1_qPCA)\n",
    "    axs[0, 1].set_title(f'{dataset_name} - qPCA Result')\n",
    "    axs[0, 1].set_xlabel('Principal Component 1')\n",
    "    axs[0, 1].set_ylabel('Principal Component 2')\n",
    "\n",
    "    axs[0, 2].bar(range(len(S_qSVD)), S_qSVD)\n",
    "    axs[0, 2].set_title(f'{dataset_name} - qSVD Singular Values')\n",
    "    axs[0, 2].set_xlabel('Index')\n",
    "    axs[0, 2].set_ylabel('Singular Value')\n",
    "\n",
    "    scatter_lda = axs[1, 0].scatter(X_selected[:, 0], X_selected[:, 1], c=y, cmap='viridis')\n",
    "    legend1_lda = axs[1, 0].legend(*scatter_lda.legend_elements(), title=\"Classes\")\n",
    "    axs[1, 0].add_artist(legend1_lda)\n",
    "    axs[1, 0].set_title(f'{dataset_name} - LDA Result')\n",
    "    axs[1, 0].set_xlabel('Linear Discriminant 1')\n",
    "    axs[1, 0].set_ylabel('Linear Discriminant 2')\n",
    "\n",
    "    axs[1, 1].hist(reconstruction_error, bins=30, edgecolor='k', alpha=0.7)\n",
    "    axs[1, 1].set_title(f'{dataset_name} - Reconstruction Error Distribution')\n",
    "    axs[1, 1].set_xlabel('Reconstruction Error')\n",
    "    axs[1, 1].set_ylabel('Frequency')\n",
    "\n",
    "    axs[1, 2].bar(['Entropy'], [quantum_entropy])\n",
    "    axs[1, 2].set_title(f'{dataset_name} - Quantum Relative Entropy')\n",
    "    axs[1, 2].set_ylabel('Entropy Value')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"f4.svg\")\n",
    "    plt.show()\n",
    "\n",
    "# Run the analysis for different datasets\n",
    "for dataset in ['iris', 'digits', 'wine', 'mnist', 'cifar10']:\n",
    "    run_analysis(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install opencv-python scikit-learn matplotlib numpy\n",
    "\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.manifold import TSNE\n",
    "from torchvision import datasets, transforms\n",
    "import cv2\n",
    "\n",
    "# Function to load MNIST dataset\n",
    "def load_mnist_data():\n",
    "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "    trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
    "    return trainset\n",
    "\n",
    "# Function to load Lenna image\n",
    "def load_lenna_image():\n",
    "    lenna_image = cv2.imread('path/to/lenna_256.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "    lenna_image = cv2.resize(lenna_image, (28, 28))\n",
    "    return lenna_image / 255.0  # Normalize the image\n",
    "\n",
    "# Load data\n",
    "mnist_data = load_mnist_data()\n",
    "lenna_image = load_lenna_image()\n",
    "\n",
    "# Sample MNIST image\n",
    "sample_mnist_image, _ = mnist_data[0]\n",
    "sample_mnist_image = sample_mnist_image.numpy()[0]\n",
    "def perform_pca(data, n_components):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    data_centered = data - np.mean(data, axis=0)\n",
    "    transformed_data = pca.fit_transform(data_centered)\n",
    "    return transformed_data, pca\n",
    "\n",
    "# Reshape images to 1D arrays and stack\n",
    "data_matrix = np.vstack([sample_mnist_image.flatten(), lenna_image.flatten()])\n",
    "\n",
    "# Perform PCA\n",
    "transformed_data, pca = perform_pca(data_matrix, n_components=2)\n",
    "\n",
    "# Visualize PCA results\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(transformed_data[:, 0], transformed_data[:, 1], marker='o')\n",
    "plt.title('PCA on MNIST and Lenna Images')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.show()\n",
    "def perform_lda(data, labels, n_components):\n",
    "    lda = LDA(n_components=n_components)\n",
    "    transformed_data = lda.fit_transform(data, labels)\n",
    "    return transformed_data, lda\n",
    "\n",
    "# Create dummy labels for MNIST and Lenna images\n",
    "labels = np.array([0, 1])\n",
    "\n",
    "# Perform LDA\n",
    "transformed_lda_data, lda = perform_lda(data_matrix, labels, n_components=1)\n",
    "\n",
    "# Visualize LDA results\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(transformed_lda_data[:, 0], np.zeros_like(transformed_lda_data[:, 0]), marker='o')\n",
    "plt.title('LDA on MNIST and Lenna Images')\n",
    "plt.xlabel('Linear Discriminant 1')\n",
    "plt.show()\n",
    "def perform_tsne(data, n_components, perplexity):\n",
    "    tsne = TSNE(n_components=n_components, perplexity=perplexity, n_iter=300)\n",
    "    transformed_data = tsne.fit_transform(data)\n",
    "    return transformed_data\n",
    "\n",
    "# Perform t-SNE\n",
    "transformed_tsne_data = perform_tsne(data_matrix, n_components=2, perplexity=2)\n",
    "\n",
    "# Visualize t-SNE results\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(transformed_tsne_data[:, 0], transformed_tsne_data[:, 1], marker='o')\n",
    "plt.title('t-SNE on MNIST and Lenna Images')\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install opencv-python scikit-learn matplotlib numpy\n",
    "\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.manifold import TSNE\n",
    "from torchvision import datasets, transforms\n",
    "import cv2\n",
    "\n",
    "# Function to load MNIST dataset\n",
    "def load_mnist_data():\n",
    "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "    trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
    "    return trainset\n",
    "\n",
    "# Function to load Lenna image\n",
    "def load_lenna_image():\n",
    "    lenna_image = cv2.imread('path/to/lenna_256.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "    lenna_image = cv2.resize(lenna_image, (28, 28))\n",
    "    return lenna_image / 255.0  # Normalize the image\n",
    "\n",
    "# Load data\n",
    "mnist_data = load_mnist_data()\n",
    "lenna_image = load_lenna_image()\n",
    "\n",
    "# Sample MNIST image\n",
    "sample_mnist_image, _ = mnist_data[0]\n",
    "sample_mnist_image = sample_mnist_image.numpy()[0]\n",
    "def perform_pca(data, n_components):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    data_centered = data - np.mean(data, axis=0)\n",
    "    transformed_data = pca.fit_transform(data_centered)\n",
    "    return transformed_data, pca\n",
    "\n",
    "# Reshape images to 1D arrays and stack\n",
    "data_matrix = np.vstack([sample_mnist_image.flatten(), lenna_image.flatten()])\n",
    "\n",
    "# Perform PCA\n",
    "transformed_data, pca = perform_pca(data_matrix, n_components=2)\n",
    "\n",
    "# Visualize PCA results\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(transformed_data[:, 0], transformed_data[:, 1], marker='o')\n",
    "plt.title('PCA on MNIST and Lenna Images')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.show()\n",
    "def perform_lda(data, labels, n_components):\n",
    "    lda = LDA(n_components=n_components)\n",
    "    transformed_data = lda.fit_transform(data, labels)\n",
    "    return transformed_data, lda\n",
    "\n",
    "# Create dummy labels for MNIST and Lenna images\n",
    "labels = np.array([0, 1])\n",
    "\n",
    "# Perform LDA\n",
    "transformed_lda_data, lda = perform_lda(data_matrix, labels, n_components=1)\n",
    "\n",
    "# Visualize LDA results\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(transformed_lda_data[:, 0], np.zeros_like(transformed_lda_data[:, 0]), marker='o')\n",
    "plt.title('LDA on MNIST and Lenna Images')\n",
    "plt.xlabel('Linear Discriminant 1')\n",
    "plt.show()\n",
    "def perform_tsne(data, n_components, perplexity):\n",
    "    tsne = TSNE(n_components=n_components, perplexity=perplexity, n_iter=300)\n",
    "    transformed_data = tsne.fit_transform(data)\n",
    "    return transformed_data\n",
    "\n",
    "# Perform t-SNE\n",
    "transformed_tsne_data = perform_tsne(data_matrix, n_components=2, perplexity=2)\n",
    "\n",
    "# Visualize t-SNE results\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(transformed_tsne_data[:, 0], transformed_tsne_data[:, 1], marker='o')\n",
    "plt.title('t-SNE on MNIST and Lenna Images')\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
